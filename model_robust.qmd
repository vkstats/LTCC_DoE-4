# Model-robust design and lack-of-fit {#sec-model-robust}

Standard optimal designs, as discussed in Week 3, suffer from a dependency on the model assumed at the design stage [@adt2007]. In one sense this is natural; it is only by leverage such prior information that we can maximise the efficiency of our experiments. But clearly, this model will usually not be known with certainty before experimentation, particularly in a study where the aim is to fit one or more empirical response surface models, and hence it is desirable to ensure some robustness to the selected model and to enable assessment of lack of fit. 

The most common design selection criteria aim at estimation or inference for the primary model (-@eq-rsm). For estimation of $\boldsymbol{\beta}_1$ when $\sigma^2$ is known, *D*- and *L*-optimality are popular that minimise the following functions of design $\mathcal{D}$, treating the intercept $\beta_0$ as a nuisance parameter: 

$$
\begin{split}
\phi_{D}(\mathcal{D}) =  \left|\left[X_1^{\mathrm{T}}\left(I_n - \frac{1}{n}J_n\right)X_1\right]^{-1}\right|\,, & \quad \mbox{($D$-optimality)} \\
\phi_L(\mathcal{D}) =  \mbox{tr}\left\{L^{\mathrm{T}}\left(X_1^{\mathrm{T}}(I_n - \frac{1}{n}J_n)X_1\right)^{-1}L\right\}\,. & \quad \mbox{($L$-optimality)}
\end{split}
$$

Here $I_n$ and $J_n$ are the $n\times n$ identity and all-ones matrices, respectively, and $L$ is a $p_1 \times p$ matrix defining $p_1$ linear combinations $L^{\mathrm{T}}\boldsymbol{\beta}_1$ of interest in the experiment. 

## Model-robust design {#sec-modrob}

@Box1959 introduced the idea of incorporation of discrepancy between the assumed response surface model and an encompassing "true" model. The encompassing model contained addition polynomial terms; here we assume there are $q$ such terms, typically higher-order monomials, possibly including interactions, which we will label $x_{i(p+1)},\ldots, x_{i(p+q)}$. The encompassing model can therefore be written as

$$
\begin{split}
    Y_i & = \beta_0 + \sum_{j = 1} ^ {p} \beta_j x_{ij} + \sum_{j = p+1} ^ {p+q} \beta_j x_{ij} +\varepsilon_i \\
    & = \beta_0 + \boldsymbol{x}_{i1}^{\mathrm{T}}\boldsymbol{\beta}_1 + \boldsymbol{x}_{i2}^{\mathrm{T}}\boldsymbol{\beta}_2 + \varepsilon_i\,,
\end{split}
$${#eq-primary_potential_model}

where $\boldsymbol{x}_{i2}^{\mathrm{T}} = (x_{i(p+1)}, \ldots, x_{i(p+q)})$ holds the additional $q$ polynomial terms, with associated parameters $\boldsymbol{\beta}_2^{\mathrm{T}} = (\beta_{p+1}, \ldots, \beta_{p+q})$. Model~(-@eq-rsm) is clearly a special case of model (-@eq-primary_potential_model) with $\boldsymbol{\beta}_2 = \boldsymbol{0}_q$. @DuMouchel1994 labelled the polynomial terms in the assumed model as **primary** and the additional terms in the encompassing model as **potential**.

One desirable aim is to be able to estimate $\boldsymbol{\beta}_1$ from model (-@eq-rsm) protected from contamination from the potential terms. Assuming least squares estimation, or equivalently maximum likelihood estimation with normal errors, it is natural to consider the mean squared error (MSE) of $\hat{\boldsymbol{\beta}}_1$ which can be characterised via the MSE matrix [@FedorovMontepiedra1997]:

$$
\begin{split}
\mbox{MSE}\left(\hat{\boldsymbol{\beta}}_1\right)& = \mathtt{E}_{\boldsymbol{Y}}[(\hat{\boldsymbol{\beta}}_1 -\boldsymbol{\beta}_1)(\hat{\boldsymbol{\beta}}_1 - \boldsymbol{\beta})_1^\top]\notag\\
& = \sigma^2[X_1^{\mathrm{T}} (I_n - \frac{1}{n}J_n) X_1]^{-1} + A_1\boldsymbol{\beta}_2\boldsymbol{\beta}_2^{\mathrm{T}} A_1^{\mathrm{T}}\,, 
\end{split}
$${#eq-MSE}

where 

$$
A_1 = \left[X_1^{\mathrm{T}} \left(I_n - \frac{1}{n}J_n\right) X_1\right]^{-1}X_1^{\mathrm{T}} \left(I_n - \frac{1}{n}J_n\right)X_2
$$ 

is the $p\times q$ alias matrix between the primary and potential terms (excluding the intercept).

An analogy of variance-based alphabetic criteria is to consider functionals of this matrix. For the determinant, letting $M = X_1^{\mathrm{T}} (I_n - \frac{1}{n}J_n) X_1$ and $\tilde{\boldsymbol{\beta}}_2 = \boldsymbol{\beta}_2 / \sigma$, we obtain

$$
\begin{split}
\left|\mbox{MSE}\left(\hat{\boldsymbol{\beta}}_1\right)\right| & = \sigma^{2p}\left| M^{-1} + A_1\tilde{\boldsymbol{\beta}}_2\tilde{\boldsymbol{\beta}}_2^{\mathrm{T}} A_1^{\mathrm{T}}\right| \\
& = \sigma^{2p}\left|M^{-1}\right|\left(1 + \tilde{\boldsymbol{\beta}}_2^{\mathrm{T}} X^{\mathrm{T}}_2X_1M^{-1}X_1^{\mathrm{T}} X_2\tilde{\boldsymbol{\beta}}_2\right)\,,
\end{split}
$$

using the matrix determinant lemma [@Harville2006matrix. p. 417]. Hence, on a log scale,

$$
\begin{split}
\log \left|\mbox{MSE}\left(\hat{\boldsymbol{\beta}}_1\right)\right| = p\log \sigma^2 + \log \left| M^{-1} \right| \\
+ \log\left(1 + \tilde{\boldsymbol{\beta}}_2^{\mathrm{T}} X^{\mathrm{T}}_2X_1M^{-1}X_1^{\mathrm{T}} X_2\tilde{\boldsymbol{\beta}}_2\right)\,.
\end{split}
$${#eq-log-det-mse}


The first summand is constant with respect to the design, and hence can be excluded from any objective function. The second summand is the (log-scale) *D*-optimality objective function, focused on precise estimation of the primary terms $\boldsymbol{\beta}_1$. And the third summand quantifies the bias introduced by not including the potential terms in the fitted model. 

The trade-off between variance and bias is clearly controlled by the relative values of $\sigma^2$ and $\boldsymbol{\beta}_2$; in particular, Equation (-@eq-log-det-mse) reduces to a form equivalent to the *D*-optimality objective function if $\boldsymbol{\beta}_2 = \boldsymbol{0}_q$.

In general, the values of $\boldsymbol{\beta}_2$ will not be known. Assuming a prior distribution for $\tilde{\boldsymbol{\beta}}_2$, the expectation of (-@eq-log-det-mse) can be approximated using Monte Carlo simulation as

$$
\begin{split}
E\left[\log \left|\mbox{MSE}\left(\hat{\boldsymbol{\beta}}_1\right)\right|\right] & = 
p\log \sigma^2 + \log \left| M^{-1} \right| \\ 
& + E\left[\log\left(1 + \tilde{\boldsymbol{\beta}}_2^\top X^\top_2X_1M^{-1}X_1^\top X_2\tilde{\boldsymbol{\beta}}_2\right)\right] \nonumber\\
& \approx p\log \sigma^2 + \log \left| M^{-1} \right| \\ 
&+ \frac{1}{B}\sum_{i=1}^B\log\left(1 + \tilde{\boldsymbol{\beta}}_{2i}^\top X^\top_2X_1M^{-1}X_1^\top X_2\tilde{\boldsymbol{\beta}}_{2i}\right)\,,
\end{split}
$${#eq-mc-mse}

where $\tilde{\boldsymbol{\beta}}_{21}, \ldots,\tilde{\boldsymbol{\beta}}_{2B}$ are a sample from the prior distribution. Here, we will use a normal distribution for $\boldsymbol{\beta}_2\sim \mathcal{N}\left(\boldsymbol{0}_q, \sigma^2\tau^2I_q\right)$ for $\tau^2>0$, leading to $\tilde{\boldsymbol{\beta}}_2\sim \mathcal{N}(\boldsymbol{0}_q,\tau^2I_q)$.

Obtaining a precise approximation via (-@eq-mc-mse) may require large values of $B$ and hence be computationally expensive. As an alternative, we can take a "locally optimal" approach and choose a point prior for $\tilde{\boldsymbol{\beta}}_2$ at which to evaluate (-@eq-log-det-mse). One possibility is to set $\boldsymbol{\beta}_2 = \pm \sigma\tau \boldsymbol{1}_q$, and hence $\tilde{\boldsymbol{\beta}}_2 = \pm \tau \boldsymbol{1}_q$. This choice fixes each potential parameter to be one standard deviation from the prior mean. Taking $\tilde{\boldsymbol{\beta}}_2 = \tau \boldsymbol{1}_q$, without loss of generality, we obtain

$$
\begin{split}
E\left[\log \left|\mbox{MSE}\left(\hat{\boldsymbol{\beta}}_1\right)\right|\right] \approx 
p\log \sigma^2 + \log \left| M^{-1} \right| \\
+ \log\left(1 + \tau^2\boldsymbol{1}_q^{\mathrm{T}} X^{\mathrm{T}}_2X_1M^{-1}X_1^{\mathrm{T}} X_2\boldsymbol{1}_q\right)\,. 
\end{split}
$${#eq-pp-mse}

We define the *MSE(D)*-criterion via minimisation of

$$
\phi_{MSE(D)}(\mathcal{D}) = \exp\left\{E\left[\log \left|\mbox{MSE}\left(\hat{\boldsymbol{\beta}}_1\right)\right|\right]\right\}\,,
$$

with the expected log determinant approximated using either (-@eq-mc-mse) or (-@eq-pp-mse).

We can also consider an *MSE(L)*-criterion formed from the trace of the MSE matrix. As a linear functional, the expectation can be found directly:

$$
\begin{split}
\phi_{MSE(L)}(\mathcal{D}) & = E\left\{\mbox{trace}\left[\mbox{MSE}\left(\hat{\boldsymbol{\beta}}_1\right)\right]\right\} \\ 
& = \mbox{trace}\left\{E\left[\mbox{MSE}\left(\hat{\boldsymbol{\beta}}_1\right)\right]\right\} \\
& = \mbox{trace}\left[\sigma^2M^{-1} + E\left(A_1\boldsymbol{\beta}_2\boldsymbol{\beta}_2^\top A_1^\top\right)\right] \\
& = \sigma^2\mbox{trace}\left[M^{-1} + \tau^2 A_1^\top A_1\right]\,.
\end{split}
$$

## Pure error and lack-of-fit

In earlier weeks of the course, the decomposition of the residual sum of squares into pure error and lack-of-fit has already been discussed. Inference for model (-@eq-rsm) also depends on the availability of an unbiased estimator for $\sigma^2$. When uncertainty about the assumed model is being acknowledged, it is important that sufficient pure error degrees of freedom exist in the design to provide this unbiased estimate. To ensure this, @GilmourandTrinca2012 suggested a class of criteria that explicitly incorporate the F-distribution quantiles on which parameter confidence regions depend. In particular, they defined *DP*- and *LP*-optimal designs that minimise

\begin{align*}
\phi_{(DP)_S}(\mathcal{D}) & =  F_{p,d;1-\alpha}^p\phi_{D}(\mathcal{D})\,, & \quad \mbox{($DP$-optimality)} \\
\phi_{LP}(\mathcal{D}) & =  F_{1,d;1-\alpha}\phi_L(\mathcal{D})\,, & \quad \mbox{($LP$-optimality)}
\end{align*}

where $d = n-t$ is the number of replicated treatments in the experiment, $\alpha$ is a pre-chosen significance level and $F_{df1, df2; 1-\alpha}$ is the quantile of an F-distribution with $df1$ and $df2$ degrees of freedom such that the probability of being less than or equal to this quantile is $1-\alpha$. 

The alternative experimental aim of model sensitivity is concerned with determining lack-of-fit in the direction of the potential terms. One route to achieving this is via alphabetic criterion based on the posterior variance-covariance matrix for $\boldsymbol{\beta}_2$, conditional on the value of $\sigma^2$, using the prior defined in @sec-modrob

\begin{align*}
\Sigma_2 & = \sigma^2\left\{X_2^{\mathrm{T}}\left[I_q - X(X^{\mathrm{T}} X)^{-1}X^{\mathrm{T}}\right]X_2 + \frac{1}{\tau^2}I_q\right\}^{-1} \\
& = \sigma^2\left(R + \frac{1}{\tau^2}I_q \right)^{-1}\,,
\end{align*}

where we assume normally-distributed errors in model (-@eq-primary_potential_model).

The ability of the design to make inference about the potential terms, and hence detect any lack of fit in the direction of model (-@eq-primary_potential_model) can be quantified via functionals of $R + \frac{1}{\tau^2}I_q$. We define Lack-of-fit *DP*- and *LP*-criteria that minimise

$$
\begin{split}
\phi_{LoF-DP}(\mathcal{D}) = F^q_{q, d; 1-\alpha_{L}} \left|R + \frac{1}{\tau^2}I_q\right|^{-1}\,, & \quad (\mbox{LoF-$DP$-optimality)} \\ 
\phi_{LoF-LP}(\mathcal{D}) = F_{1, d; 1-\alpha_{L}} \mbox{tr}\left\{L^\top\left(R + \frac{1}{\tau^2}I_q\right)^{-1}L\right\}\,. & \quad (\mbox{LoF-$LP$-optimality)}
\end{split}
$$

The inclusion of the $F$-quantiles recognises the need for a pure error estimate for $\sigma^2$ and encourages replication of treatments. Both criteria target designs with matrices $X_1$ and $X_2$ being (near) orthogonal to each other, which will also maximise the power of the lack-of-fit test for the potential terms.