[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "LTCC Design of Experiments",
    "section": "",
    "text": "Welcome\nThese lecture notes support Week 4 of the London Taught Course Centre 2024-25 advanced module on “Design of Experiments”.\nThe main focus is on two areas - experimental design under treatment interference and multi-objective experimental design - that are active research areas. As such, do not necessarily expect all the material to align with nice tidy results!",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#about",
    "href": "index.html#about",
    "title": "LTCC Design of Experiments",
    "section": "About",
    "text": "About\nI am a Lecturer in Statistics in the Department of Mathematics at King’s College London. My research interests are in the design of experiments, particularly experiments with connected units. I have recently developed a new module on Design of Experiments for King’s MSc and undergraduate programmes.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#acknowledgments",
    "href": "index.html#acknowledgments",
    "title": "LTCC Design of Experiments",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nThese notes are largely based on joint research with Professor Steve Gilmour, Dr Ben Parker, Dr Olga Egorova, Andrew Mead and Professor Luzia Trinca.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "In Weeks 1-3, you were (re-) introduced to some of the fundamental topics in design of experiments, including randomisation, factorial treatment structure, response surface methods and optimal design. To a greater or lesser degree, these ideas and principles are dependent on a number of assumptions holding:\n\nthat the expected response from a given unit only depends on the treatment applied to that unit, and not on the treatments applied to any other units (stable unit treatment value assumption);\nfor factorial, response surface and optimal designs, that a reasonable approximating statistical model can be specified that leads to unbiased estimation of quantities of interest;\nfor optimal designs, that the aim of the experiment can be neatly encapsulated in a single mathematical expression or objective function.\n\nThis week, we will focus on approaches that allow us to relax one or the other of these assumptions:\n\nin Part 1 (Experiments with interference), we will introduce methods for designing and analysing experiments when treatment interference is anticipated. In particular, with the aim of estimating both direct and indirect treatment effects, i.e., the effects both from applying the treatment to a given unit and from applying, possible different treatments, to connected units;\nin Part 2 (Multi-objective experimentation), we will introduce multi-objective (compound) design optimality criteria that address multiple experimental aims simultaneously. A particular focus will be robust designs that allow for lack-of-fit testing and acknowledge model inadequacy.\n\nBoth topics are active research areas. As such, these notes are just a snapshot summary of current work, and links will be provided to related literature.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "interference.html",
    "href": "interference.html",
    "title": "Experiments with interference",
    "section": "",
    "text": "We will start with a standard model for analysing a designed experiment,\n\\[\ny_{i} = \\mu + \\tau_{r(i)} + \\varepsilon_i\\,,\n\\tag{1}\\] with the aim of estimating treatment differences \\(\\tau_j - \\tau_k\\). Here, \\(r(i) \\in \\{1,\\ldots,t\\}\\) indicates which treatment was allocated to the \\(i\\)th unit (\\(i = 1,\\ldots,n\\)).\nThis model makes the stable unit treatment value assumption (SUTVA), which states that the response from any particular unit is unaffected by the assignment of treatments to other units (Cox, 1958).\nConsider three different experiments.\n\nA clinical trial, e.g., for an antidepressant, where different medical treatments are to be compared using a set of patients. The experiment is split into (time) periods, and within a period, each patient will be assigned one of the treatments. Across the whole experiment, each patient will be assigned all the treatments.\nAn agricultural experiment, e.g., comparing different varieties of wheat. The field available for the experiment is split into different plots, with one treatment assigned to each plot.\nA marketing experiment to assess the effectiveness of different adverts, e.g., on a social media platform. Each user on (a subset of) the platform will be shown one of the different adverts.\n\nWhat do all three of these experiments have in common? In each case, it may be necessary to consider the impact of treatment interference (or treatment carryover or spillover).\n\nThe clinical response obtained from the application of an antidepressant in a given period may also be affected by the treatment applied in the preceding period.\nThe response, e.g., crop yield, from a given plot may be affected by the variety of wheat applied to neighbouring plots, due to shading or attractiveness to pests.\nThe response from a particular social media user to an advert may be influenced by the adverts seen by their connections or friends.\n\nIgnoring substantial treatment interference, as in model (1), can lead to biased estimates of differences between the direct treatment effects \\(t_r\\).\nIn some experiments, it may be possible and sufficient to mitigate any treatment interference through adjustments to the experimental protocol; for example, by adding “wash-out” periods between treatments in the clinical trial or “guard plots” between treated plots in the agricultural experiment. But in many cases this may not be possible (is it ethical to have patients untreated in a clinical trial?) or there may be interest in the indirect effect of each treatment; for example, the viral effect of the adverts in the marketing experiment.\nHence, it is of interest to study designs and models which account for treatment interference.\n\n\n\n\nCox, D. R. (1958) Planning of Experiments. Wiley.",
    "crumbs": [
      "Experiments with interference"
    ]
  },
  {
    "objectID": "clinical.html",
    "href": "clinical.html",
    "title": "1  Clinical trials with interference",
    "section": "",
    "text": "1.1 Cross-over trials\nThe most common form of clinical trial is a parallel group trial, where each subject is allocated to a single treatment, and treatment comparisons are made between the groups. In contrast, in a cross-over trial, each subject is assigned a sequence of treatments across different periods. Interest is typically still in comparing individual treatments, not sequences, and the experimental units in such a trial, to which treatments can be applied, are the periods within each subject.\nCross-over trials are common in studies of chronic medical conditions, where repeated treatment of a disease is required. Single subject (“N of 1”) trials are a special case of cross-over trials using a single patient to determine personalised medicine results. Cross-over trials may not be appropriate for trials on acute conditions.\nThe main advantages of cross-over trials is that each subject acts as their own control, and treatment comparisons can be made “within subject” and hence are less affected by subject-to-subject variability (e.g., caused by unmeasured and unadjusted covariates). Hence, treatment comparisons could be statistically more efficient than when using a parallel group trial.\nHowever, this feature of within subject comparison can also bring disadvantages. Principally,\nIt is quite common to try to mitigate potential interference by building in wash-out periods between treatment periods in the trial, to allow the subjects to recover from each treatment. However, ensuring this is effective requires knowledge of the pharmacodynamics of the different treatments which may be unknown or uncertain. In some studies, it may not be ethical to pause treatment for a wash-out. Therefore, it is wise to plan for the possibility of interference in the design and analysis of the trial.\nMethodology for the design and analysis of cross-over trials is discussed in the book by Jones and Kenwood (2015) and the review chapter by Bose and Dey (2015).",
    "crumbs": [
      "Experiments with interference",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Clinical trials with interference</span>"
    ]
  },
  {
    "objectID": "clinical.html#cross-over-trials",
    "href": "clinical.html#cross-over-trials",
    "title": "1  Clinical trials with interference",
    "section": "",
    "text": "the order in which the treatments are applied may impact the outcome, especially if one or more of the treatments have adverse side effects.\nmost pertinently for us here, there may be treatment interference between periods of the design (typically called carry-over in a cross-over trial). That is, the outcome from period \\(i\\) may depend on the treatment applied in period \\(i-1\\) (first-order interference) or even from earlier periods.\n\n\n\n\n1.1.1 2x2 cross-over trial\nThe simplest form of cross-over design concerns \\(t=2\\) treatments and \\(p=2\\) periods, the so-called \\(2\\times 2\\) trial. There are clearly only two possible sequences, see Table 1.1, and each subject in the trial is randomised to one of the two. As discussed above, a wash-out period is usually inserted between the two treatment periods.\n\n\nCode\nAB &lt;- data.frame(\n  sequence = rep(1:2, c(2, 2)),\n  period = rep(1:2, 2),\n  trt = c(\"A\", \"B\", \"B\", \"A\")\n)\nAB |&gt; \n  pivot_wider(names_from = period, values_from = trt) |&gt;\n  kable(col.names = c(\"Sequence\", paste(\"Period\", 1:2)),\n        align = rep(\"c\", 3))\n\n\n\n\nTable 1.1: Sequences for a 2x2 cross-over trial.\n\n\n\n\n\n\nSequence\nPeriod 1\nPeriod 2\n\n\n\n\n1\nA\nB\n\n\n2\nB\nA\n\n\n\n\n\n\n\n\nAn example of a \\(2\\times 2\\) trial to investigate the efficacy of an inhaled drug (A), compared to a control (B), for patients suffering from chronic obstructive pulmonary disease (COPD). Subjects used either A or B twice daily for four weeks (period 1) before switching to the other treatment for the following four weeks. The response was the mean expiratory flow rate (PEFR) based on readings recorded each morning by the subjects. Data for 54 subjects enrolled in the trial are given in Table 1.2. The first 27 subjects had been randomised to the sequence AB, and next 27 to the sequence BA.\n\n\nCode\npefr &lt;- read_csv(here(\"data\", \"pefr.csv\"), show_col_types = FALSE) |&gt;\n  clean_names() |&gt;\n  slice(1:54)\npefr_AB &lt;- pefr |&gt;\n  filter(sequence == \"AB\")\npefr_BA &lt;- pefr |&gt;\n  filter(sequence == \"BA\") \nbind_cols(pefr_AB, pefr_BA) |&gt;\n  select(c(1, 3, 4, 5, 7, 8)) |&gt;\n  kable(col.names = rep(c(\"Subject\", \"Period 1\", \"Period 2\"), 2)) |&gt;\n  add_header_above(c(\"Sequence AB\" = 3, \"Sequence BA\" = 3))\n\n\n\n\nTable 1.2: Mean morning PEFR (L/min) from a \\(2\\times 2\\) cross-over trial (adapted from Jones and Kenwood, 2015, ch. 2).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSequence AB\n\n\nSequence BA\n\n\n\nSubject\nPeriod 1\nPeriod 2\nSubject\nPeriod 1\nPeriod 2\n\n\n\n\n1\n121.905\n116.667\n28\n138.333\n138.571\n\n\n2\n218.500\n200.500\n29\n225.000\n256.250\n\n\n3\n235.000\n217.143\n30\n392.857\n381.429\n\n\n4\n250.000\n196.429\n31\n190.000\n233.333\n\n\n5\n186.190\n185.500\n32\n191.429\n228.000\n\n\n6\n231.563\n221.842\n33\n226.190\n267.143\n\n\n7\n443.250\n420.500\n34\n201.905\n193.500\n\n\n8\n198.421\n207.692\n35\n134.286\n128.947\n\n\n9\n270.500\n213.158\n36\n238.000\n248.500\n\n\n10\n360.476\n384.000\n37\n159.500\n140.000\n\n\n11\n229.750\n188.250\n38\n232.750\n276.563\n\n\n12\n159.091\n221.905\n39\n172.308\n170.000\n\n\n13\n255.882\n253.571\n40\n266.000\n305.000\n\n\n14\n279.048\n267.619\n41\n171.333\n186.333\n\n\n15\n160.556\n163.000\n42\n194.737\n191.429\n\n\n16\n172.105\n182.381\n43\n200.000\n222.619\n\n\n17\n267.000\n313.000\n44\n146.667\n183.810\n\n\n18\n230.750\n211.111\n45\n208.000\n241.667\n\n\n19\n271.190\n257.619\n46\n208.750\n218.810\n\n\n20\n276.250\n222.105\n47\n271.429\n225.000\n\n\n21\n398.750\n404.000\n48\n143.810\n188.500\n\n\n22\n67.778\n70.278\n49\n104.444\n135.238\n\n\n23\n195.000\n223.158\n50\n145.238\n152.857\n\n\n24\n325.000\n306.667\n51\n215.385\n240.476\n\n\n25\n368.077\n362.500\n52\n306.000\n288.333\n\n\n26\n228.947\n227.895\n53\n160.526\n150.476\n\n\n27\n236.667\n220.000\n54\n353.810\n369.048\n\n\n\n\n\n\n\n\n\n\nThe traditional linear model used with cross-over experiments contains terms corresponding to subjects, period, treatment and interference:\n\\[\n\\begin{split}\ny_{ij} = \\mu + \\alpha_i + \\beta_j + \\tau_{r(i,j)} + \\rho_{r(i-1,j)} + \\varepsilon_{ij}\\,,\\\\ i = 1,\\ldots,p;\\, j = 1,\\ldots, n\\,,\n\\end{split}\n\\tag{1.1}\\]\nwhere \\(y_{ij}\\) is the response from the \\(j\\)th subject in the \\(i\\)th period, \\(r(i,j) \\in \\{1,\\ldots,t\\}\\) denotes the treatment allocated to the \\(j\\)th subject in the \\(i\\)th period, \\(\\mu\\) is a general mean, \\(\\alpha_i\\) is the \\(i\\)th period effect, \\(\\beta_j\\) is the \\(j\\)th subject effect, \\(\\tau_{r(i,j)}\\) is the direct effect of treatment \\(r(i,j)\\) and \\(\\rho_{r(i-1,j)}\\) is the interference (indirect, carry-over) effect of treatment \\(r(i-1,j)\\). We define indirect effect \\(\\rho_{r(0,j)}=0\\), for \\(j=1,\\ldots,n\\), to reflect the lack of interference in period 1. The error terms \\(\\varepsilon_{ij}\\) are uncorrelated random variables with zero mean and constant variance \\(\\sigma^2\\).\nBefore we fit model (1.1), it is informative to fit related models corresponding to different experiment formulations and assumptions. We start by fitting the model\n\n\\[\n\\begin{split}\ny_{ij} = \\mu + \\alpha_i + \\psi_{k} + \\beta_{j^\\prime|k} +  + \\varepsilon_{ij}\\,,\\\\\ni = 1,\\ldots,p;\\, j^\\prime = 1,\\ldots, n/s;\\, k = 1,\\ldots, s\\,,\n\\end{split}\n\\] which investigates the effects \\(\\psi_k\\) of the \\(s\\) sequences in the design, as a significant sequence effect, adjusted for a period effect, arises from an indirect treatment effect. Here, subject is now nested within sequence. This is equivalent to a parallel group trial to compare sequences on 54 subjects and hence, as sequences can only be compared between subjects, the correct error mean square for testing for a sequence difference is the between subjects residual, with 53 degrees of freedom, see Table 1.3. There is no evidence of a difference between sequences or, equivalently, no difference in interference between the two treatments. However, in general this test usually has very low power due to large subjec-to-subject variation. Hence we must be cautious in assuming a lack of statistical significance also means a lack of substantive interference. For the PEFR experiment, the clinicians did not expect treatment interference and hence we take lack of statistical significance as evidence for no carry-over.\nNote that the test in Table 1.3 is (indirectly) testing \\(\\rho_1 = \\rho_2\\), and not \\(\\rho_1=\\rho_2=0\\). Hence, conclusions are being drawn about equality of interference from the two treatments, not about the presence or absence of treatment interference.\n\n\n\nCode\npefr_long &lt;- pefr |&gt; \n  pivot_longer(cols = starts_with(\"period\"), names_to = \"period\", values_to = \"pefr\")\npefr_long &lt;-  pefr_long |&gt;\n  mutate(trt = case_when(\n    sequence == \"AB\" & period == \"period_1\" ~ \"A\",\n    sequence == \"AB\" & period == \"period_2\" ~ \"B\",\n    sequence == \"BA\" & period == \"period_1\" ~ \"B\",\n    .default = \"A\"\n    ) \n  ) |&gt;\n  mutate(subject = as.factor(subject))\npefr_parallel &lt;- aov(pefr ~ sequence + period + Error(subject), data = pefr_long)\ntidy(pefr_parallel) |&gt;\n  mutate(term = case_when(\n                          row_number() == 2 ~ \"between subject residual\",\n                          row_number() == 4 ~ \"within subject residual\",\n                          .default = term)) |&gt;\n  select(-stratum) |&gt;\n  mutate(across(where(is.numeric), ~ round(.x, digits = 3))) |&gt;\n  kable(col.names = c(\"\", \"df\", \"Sum Sq.\", \"Mean Sq.\", \"F-value\", \"P-value\"))\n\n\n\n\nTable 1.3: Analysis of variance from PEFR parallel trial\n\n\n\n\n\n\n\ndf\nSum Sq.\nMean Sq.\nF-value\nP-value\n\n\n\n\nsequence\n1\n21834.440\n21834.440\n2.013\n0.162\n\n\nbetween subject residual\n52\n563973.220\n10845.639\n\n\n\n\nperiod\n1\n313.444\n313.444\n0.825\n0.368\n\n\nwithin subject residual\n53\n20144.971\n380.094\n\n\n\n\n\n\n\n\n\n\nA standard two period crossover trial cannot allow for complete balance in the sense of each treatment following every other including itself (we have no cases of treatment A following A or B following B). Hence, when testing for a treatment difference we do need the assumption of no interference, \\(\\rho_{1} = \\rho_2\\), to ensure the least squares estimate \\(\\hat{\\tau}_1 - \\hat{\\tau}_2\\) is unbiased. If there is evidence of unequal treatment interference, we can use the data from one period only and estimate the treatment difference as if from a parallel group design. However, such an analysis will have much lower power.\nAssuming we are happy with the evidence supporting assumption of equal treatment interference, the experiment could also be analysed as a row-column design, using\n\\[\ny_{ij} = \\mu + \\alpha_i + \\beta_j + \\tau_{r(i,j)} + \\varepsilon_{ij}\\,,\\quad i = 1,\\ldots,p;\\, j = 1,\\ldots, n\\,.\n\\] The corresponding analysis of variance is given in Table 1.4, and shows a significant difference between treatments A and B.\n\n\nCode\npefr_rc &lt;- aov(pefr ~ period + trt + Error(subject), data = pefr_long)\ntidy(pefr_rc) |&gt;\n  mutate(term = case_when(\n                          row_number() == 1 ~ \"subjects\",\n                          row_number() == 3 ~ \"treatment\",\n                          .default = term)) |&gt;\n  select(-stratum) |&gt;\n  mutate(across(where(is.numeric), ~ round(.x, digits = 3))) |&gt;\n  kable(col.names = c(\"\", \"df\", \"Sum Sq.\", \"Mean Sq.\", \"F-value\", \"P-value\"))\n\n\n\n\nTable 1.4: Analysis of variance from PEFR row-column trial\n\n\n\n\n\n\n\ndf\nSum Sq.\nMean Sq.\nF-value\nP-value\n\n\n\n\nsubjects\n53\n585807.660\n11052.975\n\n\n\n\nperiod\n1\n313.444\n313.444\n0.936\n0.338\n\n\ntreatment\n1\n2723.059\n2723.059\n8.128\n0.006\n\n\nResiduals\n52\n17421.912\n335.037\n\n\n\n\n\n\n\n\n\n\nBringing together these different aspects, model (1.1) can be fitted, resulting in the analysis of variance in Table 1.5 combining the tests performed above.\n\n\nCode\npefr_co &lt;- aov(pefr ~ sequence + period + trt + Error(subject), data = pefr_long)\ntidy(pefr_co) |&gt;\n  mutate(term = case_when(\n                          row_number() == 1 ~ \"sequence (interference)\",\n                          row_number() == 2 ~ \"between subject residual\",\n                          row_number() == 4 ~ \"treatment\",\n                          row_number() == 5 ~ \"within subject residual\",\n                          .default = term)) |&gt;\n  select(-stratum) |&gt;\n  mutate(across(where(is.numeric), ~ round(.x, digits = 3))) |&gt;\n  kable(col.names = c(\"\", \"df\", \"Sum Sq.\", \"Mean Sq.\", \"F-value\", \"P-value\"))\n\n\n\n\nTable 1.5: Analysis of variance from PEFR cross-over trial\n\n\n\n\n\n\n\ndf\nSum Sq.\nMean Sq.\nF-value\nP-value\n\n\n\n\nsequence (interference)\n1\n21834.440\n21834.440\n2.013\n0.162\n\n\nbetween subject residual\n52\n563973.220\n10845.639\n\n\n\n\nperiod\n1\n313.444\n313.444\n0.936\n0.338\n\n\ntreatment\n1\n2723.059\n2723.059\n8.128\n0.006\n\n\nwithin subject residual\n52\n17421.912\n335.037\n\n\n\n\n\n\n\n\n\n\n\n\n1.1.2 Larger cross-over trials\nThe design of a \\(2\\times 2\\) trial is very simple. For larger numbers of treatments and periods, decisions are required on which sequences should be used.\nA balanced cross-over design has each treatment occuring the same number of times in each period, and each treatment follows every other treatment the same number of times, with no treatment following itself. Such a design has the property that every pair of estimated direct treatment differences, \\(\\hat{\\tau}_k = \\hat{\\tau}_l\\) has the same variance.\nFor \\(p=t\\) (number of periods equal to number of treatments), such designs can be constructed by combining complete sets of \\(t-1\\) orthogonal Latin squares (John, 1971, ch. 6), see Table 1.6 for the design when \\(p=t=4\\).\n\n\nCode\nABCD1 &lt;- data.frame(\n  sequence = rep(1:4, rep(4, 4)),\n  period = rep(1:4, 4),\n  trt = c(\"A\", \"B\", \"C\", \"D\",\n          \"B\", \"A\", \"D\", \"C\",\n          \"C\", \"D\", \"A\", \"B\",\n          \"D\", \"C\", \"B\", \"A\")\n)\nABCD2 &lt;- data.frame(\n  sequence = rep(5:8, rep(4, 4)),\n  period = rep(1:4, 4),\n  trt = c(\"A\", \"D\", \"B\", \"C\",\n          \"B\", \"C\", \"A\", \"D\",\n          \"C\", \"B\", \"D\", \"A\",\n          \"D\", \"A\", \"C\", \"B\")\n)\nABCD3 &lt;- data.frame(\n  sequence = rep(9:12, rep(4, 4)),\n  period = rep(1:4, 4),\n  trt = c(\"A\", \"C\", \"D\", \"B\",\n          \"B\", \"D\", \"C\", \"A\",\n          \"C\", \"A\", \"B\", \"D\",\n          \"D\", \"B\", \"A\", \"C\")\n)\nbind_rows(ABCD1, ABCD2, ABCD3) |&gt;\n  pivot_wider(names_from = period, values_from = trt) |&gt;\n  kable(col.names = c(\"Sequence\", paste(\"Period\", 1:4)),\n        align = rep(\"c\", 5)) |&gt;\n  row_spec(c(4, 8), extra_css = \"border-bottom: 2px solid\") |&gt;\n  row_spec(c(4, 8), hline_after = TRUE)\n\n\n\n\nTable 1.6: Orthogonal Latin square design for four treatments.\n\n\n\n\n\n\nSequence\nPeriod 1\nPeriod 2\nPeriod 3\nPeriod 4\n\n\n\n\n1\nA\nB\nC\nD\n\n\n2\nB\nA\nD\nC\n\n\n3\nC\nD\nA\nB\n\n\n4\nD\nC\nB\nA\n\n\n5\nA\nD\nB\nC\n\n\n6\nB\nC\nA\nD\n\n\n7\nC\nB\nD\nA\n\n\n8\nD\nA\nC\nB\n\n\n9\nA\nC\nD\nB\n\n\n10\nB\nD\nC\nA\n\n\n11\nC\nA\nB\nD\n\n\n12\nD\nB\nA\nC\n\n\n\n\n\n\n\n\n\n\nHowever, designs constructed in this way require a large number of sequences and are not very flexible. They may also lead to unncessarily large designs; e.g., using the squares from Table 1.6 requires the total number of subjects to be a multiple of 12. Williams (1949) showed that balanced designs could be achieved using much smaller numbers of sequences. An example for \\(p=t=4\\) with only four sequences is given in Table 1.7. This design uses many fewer sequences and could be employed with the number of subjects being a multiple of four.\n\n\nCode\nbal_ABCD &lt;- data.frame(\n  sequence = rep(1:4, rep(4, 4)),\n  period = rep(1:4, 4),\n  trt = c(\"A\", \"D\", \"B\", \"C\",\n          \"B\", \"A\", \"C\", \"D\",\n          \"C\", \"B\", \"D\", \"A\",\n          \"D\", \"C\", \"A\", \"B\")\n)\nbal_ABCD |&gt;\n  pivot_wider(names_from = period, values_from = trt) |&gt;\n  kable(col.names = c(\"Sequence\", paste(\"Period\", 1:4)),\n        align = rep(\"c\", 5))\n\n\n\n\nTable 1.7: Balanced Latin square design for four treatments.\n\n\n\n\n\n\nSequence\nPeriod 1\nPeriod 2\nPeriod 3\nPeriod 4\n\n\n\n\n1\nA\nD\nB\nC\n\n\n2\nB\nA\nC\nD\n\n\n3\nC\nB\nD\nA\n\n\n4\nD\nC\nA\nB\n\n\n\n\n\n\n\n\nSuch balanced designs still cannot orthogonally estimate the direct and indirect treatment effects, as no treatment is followed by itself. A strongly balanced design is defined as one in which every treatment followed by every other treatment, including itself. In such a design, direct and indirect effects are estimated independently, simplifying the analysis and interpretation, and lowering the variance of estimators of the indirect effects.\nIf more than \\(t\\) periods are possible, a simple way to generate such a design is to add an extra period that repeats the last period in a design with \\(p=t\\) Lucas (1957), see the example in Table 1.8.\n\n\nCode\nstrbal_ABCD &lt;- data.frame(\n  sequence = rep(1:4, rep(5, 4)),\n  period = rep(1:5, 4),\n  trt = c(\"A\", \"D\", \"B\", \"C\", \"C\",\n          \"B\", \"A\", \"C\", \"D\", \"D\",\n          \"C\", \"B\", \"D\", \"A\", \"A\",\n          \"D\", \"C\", \"A\", \"B\", \"B\")\n)\nstrbal_ABCD |&gt;\n  pivot_wider(names_from = period, values_from = trt) |&gt;\n  kable(col.names = c(\"Sequence\", paste(\"Period\", 1:5)),\n        align = rep(\"c\", 6))\n\n\n\n\nTable 1.8: Strongly balanced extra-period design for four treatments.\n\n\n\n\n\n\nSequence\nPeriod 1\nPeriod 2\nPeriod 3\nPeriod 4\nPeriod 5\n\n\n\n\n1\nA\nD\nB\nC\nC\n\n\n2\nB\nA\nC\nD\nD\n\n\n3\nC\nB\nD\nA\nA\n\n\n4\nD\nC\nA\nB\nB\n\n\n\n\n\n\n\n\nMost standard construction methods for cross-over trials are combinatoric. However, designs can also be found via numerical optimisation, based on some optimality criteria. Often, this is the only approach for arbitrary numbers of treatments and periods, and for extensions to the traditional linear model (e.g., using mixed effect models).",
    "crumbs": [
      "Experiments with interference",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Clinical trials with interference</span>"
    ]
  },
  {
    "objectID": "clinical.html#interference-in-other-trials",
    "href": "clinical.html#interference-in-other-trials",
    "title": "1  Clinical trials with interference",
    "section": "1.2 Interference in other trials",
    "text": "1.2 Interference in other trials\nInterference can also occur in other trials, including parallel group trials without repeated treatment applications1. Many examples occur in assessing public health interventions, where treatments are applied in the community and community members may interact. The links governing the interference may be much more complex than the simple, directed links in a cross-over trial.\nOne mitigating strategy is the application of a cluster randomised trial (CRT), where all subjects in a cluster or group recieve the same treatment. While there are many other reasons to conduct a CRT (e.g., for practical or ethical purposes), typically clusters will be designed to limit possible treatment contamination between clusters (e.g., due to geographical distance). Within a cluster, all subjects will have received the same treatment, limiting the impact of interference.\nIn other trials, the indirect effect of each treatment may be of interest in itself. A common case are vaccine trials, where the indirect protection provided by the vaccine is important. Then, methods are required for the design and modelling of experiments to estimate indirect and total effects (Hudgens and Halloran, 2008).\n\n\n\n\nBose, M. and Dey, A. (2015) Crossover designs. In Handbook of the Design and Analysis of Experiments, pp. 159–196.\n\n\nHudgens, M. G. and Halloran, M. E. (2008) Toward causal inference with interference. Journal of the American Statistical Association, 103, 832–842.\n\n\nJohn, P. W. (1971) Statistical Design and Analysis of Experiments. Macmillan.\n\n\nJones, B. and Kenwood, M. G. (2015) Design and Analysis of Cross-over Trials. Chapman & Hall/CRC Press.\n\n\nLucas, H. L. (1957) Extra-period latin-square change over designs. Journal of Dairy Science, 40, 225–239.\n\n\nWilliams, E. J. (1949) Experimental designs balanced for the estimation of residual effects of treatments. Australian Journal of Scientific Research, 2, 149–168.",
    "crumbs": [
      "Experiments with interference",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Clinical trials with interference</span>"
    ]
  },
  {
    "objectID": "clinical.html#footnotes",
    "href": "clinical.html#footnotes",
    "title": "1  Clinical trials with interference",
    "section": "",
    "text": "https://clusterrandomisedtrials.qmul.ac.uk↩︎",
    "crumbs": [
      "Experiments with interference",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Clinical trials with interference</span>"
    ]
  },
  {
    "objectID": "networks.html",
    "href": "networks.html",
    "title": "2  Networked experiments",
    "section": "",
    "text": "2.1 Preliminaries\nA variety of different experiments from outside the clinical arena can also violate SUTVA due to treatment interference arising from connections between experimental units. One common example is online controlled experiments performed, e.g., by technology companies to assess features on websites and social media platforms (Larsen et al., 2024). Connections between users on these platforms can lead to treatment interference, e.g., by sharing positive or negative experiences. In this vein, Figure 2.1 shows a small subset of the Facebook network, comprising 2514 connections between 324 user. Experimentation on such a network via the application of treatments to users would be susceptible to interference, e.g. via discussions using instant messaging.\nFurther, there could be distinct communities within the network, with more similar responses expected from users in the same community as opposed to from different communities. In the Facebook network, example communities are indicated by difference colours. Such communities should be incorporated into the design and analysis as a blocking factor.\nIn addition to the technology industry, networked experiments can be found in many other fields. Indicative examples are given in Table 2.1, along with exemplar interventions and connections. One commong feature of these experiments, in common with the clinical trials in Chapter 1: they all involve people (or animals). In general, it is harder to harder to enforce the assumption of SUTVA on experiments in the human sciences as opposed to, for example, the lab-based or physical sciences.\nSuppose that \\(n\\) experimental units are available for experimentation, formed into a network with connections representing possible routes for treatment interference. This network can be represented as a graph \\(\\mathcal{G} = (\\mathcal{V}, \\mathcal{E})\\) with vertex set \\(\\mathcal{V}\\) representing the units and edge set \\(\\mathcal{E}\\), of size \\(l\\), representing the connections between them. The units might be users of a social network, patients in a clinical setting or plots of agricultural land.\nThese connections can be succinctly represented via the adjacency matrix, \\(A = [A]_{jh}\\), an \\(n\\times n\\) matrix with \\(A_{jh} = \\in [0, 1]\\) representing the strength of connection between nodes (units) \\(j\\) and \\(h\\). We focus on undirected graphs with \\(A_{jh} = A_{hj}\\). Lack of an edge between nodes \\(j\\) and \\(h\\) in \\(\\mathcal{E}\\) leads to \\(A_{jh} = A_{hj} = 0\\); by convention, \\(A_{jj}=0\\). A simple example is given in Table 2.2 and Figure 2.2.\nCode\nA &lt;- matrix(\nc(0, 1, 0, 1, 1,\n  1, 0, 1, 0, 0,\n  0, 1, 0, 1, 0,\n  1, 0, 1, 0, 1,\n  1, 0, 0, 1, 0),\nnrow = 5, byrow = T\n)\ncolnames(A) &lt;- rownames(A) &lt;- LETTERS[1:5]\nA |&gt;\n  kable()\n\n\n\n\nTable 2.2: Example adjacency matrix for the graph \\(\\mathcal{G}\\) in Figure 2.2 with \\(|\\mathcal{V}| = 5\\) vertices and \\(|\\mathcal{E}| = 6\\) undirected edges.\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\n\n\n\n\nA\n0\n1\n0\n1\n1\n\n\nB\n1\n0\n1\n0\n0\n\n\nC\n0\n1\n0\n1\n0\n\n\nD\n1\n0\n1\n0\n1\n\n\nE\n1\n0\n0\n1\n0\nCode\nA |&gt;\n  graph_from_adjacency_matrix(mode = \"undirected\") |&gt;\n  plot(vertex.color = c(\"orange\", \"lightblue\", \"lightblue\", \"orange\", \"orange\"))  \n\n\n\n\n\n\n\n\nFigure 2.2: Example graph \\(\\mathcal{G}\\) for the adajcency matrix in Table 2.2 with \\(|\\mathcal{V}| = 5\\) vertices and \\(|\\mathcal{E}| = 6\\) undirected edges. Colours indicate an examplar blocking into two groups.\nFor some applications, necessary blocking factors may be obvious or based on covariates external to the graph, e.g., age or sex. The colours in Figure 2.2 illustrate such a case; see also Section 2.3. For others, it may be necessary or desirable to base the blocks on the graph structure itself, e.g., using spectral clustering (Koutra et al., 2021).",
    "crumbs": [
      "Experiments with interference",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Networked experiments</span>"
    ]
  },
  {
    "objectID": "networks.html#optimal-designs-for-networked-experiments",
    "href": "networks.html#optimal-designs-for-networked-experiments",
    "title": "2  Networked experiments",
    "section": "2.2 Optimal designs for networked experiments",
    "text": "2.2 Optimal designs for networked experiments\n\n2.2.1 Linear network model\nThe adjacency matrix can be used to incorporate indirect treatment effects into a model for the experiment. Parker et al. (2016) and Koutra et al. (2021) developed the linear network effects model, where the response from unit \\(j\\) in block \\(i\\) is modelled with additive block, direct and indirect treatment effects:\n\\[\n\\begin{split}\ny_{ij} = \\mu  + \\beta_i + \\tau_{r(i,j)} + \\sum_{g=1}^{b}\\sum_{h=1}^{n_{g}} A_{\\left\\{ij,gh\\right\\}}\\gamma_{r(g,h)} +\\varepsilon_{ij}\\,, \\\\\n\\quad i=1,\\ldots,b\\,,\\,j = 1,\\ldots, n_i\\,.\n\\end{split}\n\\tag{2.1}\\]\nHere, \\(\\beta_i\\) is the \\(i\\)th block effect, \\(r(i,j)\\) is the treatment applied to the \\(j\\)th unit in the \\(i\\)th block, \\(\\tau_k\\) and \\(\\gamma_k\\) are the direct and indirect effects, respectively, of treatment \\(k\\in\\{1,\\ldots,t\\}\\). Use of the adjacency matrix in 2.1 picks out the correct indirect treatment effects, corresponding to the treatments allocated to connected units.\nFor the graph in Figure Figure 2.2, with adjacency matrix in Table 2.2, the response from node A, the first unit in block 1, would be modelled as:\n\\[\ny_{11} = \\mu + \\beta_1 + \\tau_{r(1,1)} + \\gamma_{r(1,2)} + \\gamma_{r(1,3)} + \\gamma_{r(2,1)} + \\varepsilon_{11}\\,,\n\\] with the indirect treatment effects resulting from the edges between node A and nodes D and E (in block 1) and node B (in block 2). The linear network effects model can be estimated using least squares or maximum likelihood, subject to applying suitable parameter constraints or defining a set of estimable contrasts.\n\n\n2.2.2 Optimality criteria\nTwo possible aims from the experiment are\n\nestimation of pairwise differences between direct treatment effects, or\nestimate of pairwise differences between indirect treatment effects, if primary interest is in the viral effects of a treatment.\n\nIn either case, design selection will be based on model (2.1) with direct and indirect treatment effects being mutually adjusted.\nFor efficient estimation of direct treatment differences, designs are sought that minimise the average variance of the pairwise differences:\n\\[\n\\phi_{\\tau}=\\frac{2}{t(t-1)} \\sum_{s=1}^{t-1}\\sum_{s'=s+1}^t \\text{var}(\\widehat{\\tau_s-\\tau_{s'}})\\,.\n\\tag{2.2}\\]\nSimilarly, we can define a criterion for efficient estimation of indirect treatment differences, that minimises\n\\[\n\\phi_{\\gamma}=\\frac{2}{t(t-1)} \\sum_{s=1}^{t-1}\\sum_{s'=s+1}^t \\text{var}(\\widehat{\\gamma_s-\\gamma_{s'}})\\,.\n\\tag{2.3}\\]\nIf alternative estimable contrasts were of interest, corresponding design selection criteria could be constructed. Designs can be found via application of standard optimisation algorithms, such as point exchange (Cook and Nachtsheim, 1980).\n\n\n2.2.3 Examples\nAs a first example, consider the network shown in Figure 2.3, which describes co-authorship links between academics within a university research group (Koutra et al., 2021). There are 22 nodes, split into three blocks, and 27 edges. Interest lies in estimating the effects of two treatments.\n\n\n\n\n\n\nFigure 2.3: Block designs for a co-authorship network with colours indicating blocks (identified via spectral clustering) and plotting symbol indicating allocation to treatment 1 or 2. Left: optimal design for estimation of direct effects. Right: optimal design for estimation of indirect effects.\n\n\n\nThe left-hand plot shows the design for estimating the direct effects, with the right-hand plot giving the design for estimating the indirect effects. The designs are not the same, and in general the structure will depend on the network. The \\(\\phi_{\\tau}\\)-optimal design is balanced, with equal replication of each treatment. Treatment allocation is also balanced within each block. Nodes allocated to each treatment have similar first- and second-order degrees (numbers of connections between nodes of distance one or two). Treatment allocation in the \\(\\phi_{\\gamma}\\)-optimal design is highly dependent on the network, with highly connected nodes tending to receive a different treatment from their surrounding, less connected, nodes.\nQuantitative comparisons can be made between the block network designs (BNDs) from Figure 2.3 and the optimal designs that would result from models that\n\nignore blocks and network structure (CRD: completely randomised design);\nignore network structure (RBD: randomised block design);\nignore blocks (LND: linear network design).\n\nTable 2.3 gives the efficiencies for estimating direct effects for these various designs under the corresponding models. Each row corresponds to one model: CRM (ignoring blocks and network), RBM (ignoring the network), LNM (ignoring blocks) and BNM (including blocks and network structure). Efficiencies are calculated within row; comparisons between rows are not meaningful due to the differing numbers of paramters in the different models.\n\n\nCode\ndir_eff &lt;- data.frame(\n  eff = c(1, 1, 1, 1, .89, 1, 0.68, 1, 0.86, 0.83, 1, 1, 0.73, 0.81, 0.5, 1),\n  Model = rep(c(\"CRM\", \"RBM\", \"LNM\", \"BNM\"), rep(4, 4)),\n  des = rep(c(\"CRD\", \"RBD\", \"LND\", \"BND\"), 4)\n)\ndir_eff |&gt; \n  pivot_wider(names_from = des, values_from = eff) |&gt;\n  kable() |&gt;\n  add_header_above(c(\"\", \"Designs\" = 4))\n\n\n\n\nTable 2.3: Efficiencies for estimating the direct treatment effects for designs with and without blocking and/or indirect effects under different model assumptions.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDesigns\n\n\n\nModel\nCRD\nRBD\nLND\nBND\n\n\n\n\nCRM\n1.00\n1.00\n1.00\n1\n\n\nRBM\n0.89\n1.00\n0.68\n1\n\n\nLNM\n0.86\n0.83\n1.00\n1\n\n\nBNM\n0.73\n0.81\n0.50\n1\n\n\n\n\n\n\n\n\n\n\nTwo features stand out from Table 2.3.\n\nThe importance of including blocks in the optimal design, if they are present in the model. For example, the LND is only 50% efficient if blocks are added to the model. This is because balance within blocks is not achieved by the LND.\nThe substantial loss in efficiency if network effects are excluded; e.g., the CRD and RBD lose around ~15-25% efficiency compared to the LND/BND.\n\nTable 2.4 gives efficiencies for estimating the indirect effects for the four different designs. The loss of efficiency for designs that ignore network structure is now stark (&gt;80%).\n\n\nCode\ndir_eff &lt;- data.frame(\n  eff = c(0.16, 0.12, 1, 0.64, 0.16, 0.16, 0.39, 1),\n  Model = rep(c(\"LNM\", \"BNM\"), rep(4, 2)),\n  des = rep(c(\"CRD\", \"RBD\", \"LND\", \"BND\"), 2)\n)\ndir_eff |&gt; \n  pivot_wider(names_from = des, values_from = eff) |&gt;\n  kable() |&gt;\n  add_header_above(c(\"\", \"Designs\" = 4))\n\n\n\n\nTable 2.4: Efficiencies for estimating the indirect treatment effects for designs with and without blocking under different model assumptions.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDesigns\n\n\n\nModel\nCRD\nRBD\nLND\nBND\n\n\n\n\nLNM\n0.16\n0.12\n1.00\n0.64\n\n\nBNM\n0.16\n0.16\n0.39\n1.00\n\n\n\n\n\n\n\n\n\n\nAs a second example, we briefly return to the Facebook network in Figure 2.1. For designs to again estimate two treatments, we find\n\nfor this network, the CRD and RBD are a little more efficient under \\(\\phi_\\tau\\) than in the first example, with average efficienices greater than 90%;\nunder \\(\\phi_\\gamma\\), are again very poor with efficiencies less than 20%.\n\nFor both examples, we see that ignoring important indirect effects resulting from network structure in the design of the experiment will reduce the efficiency of direct treatment effect estimation, and almost prevent effective estimation of any indirect effects.",
    "crumbs": [
      "Experiments with interference",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Networked experiments</span>"
    ]
  },
  {
    "objectID": "networks.html#sec-field",
    "href": "networks.html#sec-field",
    "title": "2  Networked experiments",
    "section": "2.3 Case study: agricultural experiment",
    "text": "2.3 Case study: agricultural experiment\nFormal design of experiments methodology was largely developed in the context of agricultural field trials, especially at the Rothamsted research station at Harpenden. This is also an application area where he potential impact of treatment interference has long been recognised and various methods for both modelling and mitigation proposed and applied. In particular, the impact of neighbouring plots was been widely considered, including through indirect treatment effects (Besag and Kempton, 1986). A typical layout of a field trial is shown in Figure 2.4, clearly showing the proximity of neighbouring plots.\n\n\n\n\n\n\nFigure 2.4: Example field layout, as used at Rothamsted\n\n\n\nWe consider experiments conducted at Rothamsted to study the differences in natural cereal aphid colonization of 21 different wheat varieties (Koutra et al., 2023). Aphids are small insects such on plant sap and can cause considerable crop damage. The experiment was performed in a \\(14\\times 6\\) grid of 1m x 1m plots, see Figure 2.6 which also gives the allocation of the 21 varieties to plots for an experiment performed in 2016. There are sufficient plots for each treatment to be replicated four times.\n\n\n\n\n\n\nFigure 2.5: Plot layout for the agricultural example with treatment allocation from the 2016 design. Numbers indicated treatments allocated to each plot.\n\n\n\nTreatment interference was thought possible due to the differing levels of susceptibility of different varieties and the strong possibility of aphids moving from plot to plot. Hence, a plot where a very susceptible variety has been planted may lead to neighbouring plots suffering from direct treatment interference. Differing structures governing this interference were considered, represented as graphs. Here, we focus on network illustrated in Figure 2.6, where the connections between (nodes) plots are inversely weighted by spatial distance.\n\n\n\n\n\n\nFigure 2.6: Network and optimal design for the wheat field trial. Numbers indicated treatments allocated to each plot.\n\n\n\nIn addition to direct and indirect treatment effects, the analysis of the experiment needed to account for the spatial structure through the inclusion of blocking factors and row-column effects. The following model was assumed,\n\\[\n\\begin{aligned}\ny_j &=\\mu+\\tau_{r(j)}+ R_i+C_k+(RC)_{ik}+r_{ig}+c_{kh} \\\\\n& +\\left(rC\\right)_{igk}+\\left(Rc\\right)_{ikh} +\\sum_{j'} A_{jj'}  \\gamma_{r(j')} + \\varepsilon_{j}\\,,\n\\end{aligned}\n\\tag{2.4}\\]\nwith \\(R\\), \\(C\\) and \\(RC\\) representing the effects of super-rows and super-columns, and their interaction (together representing super-blocks). Effects \\(r\\) and \\(c\\) are of rows and columns nested inside super-blocks. An analysis of variance based on this model for the 2016 experiment indicated the possibility of treatment interference, with indirect effects significant even after adjusted for direct treatment effects, see Table 2.5. Comparison 1 tests direct effects adjusted for indirect effects, treating the latter as nuisance factors. This comparison is the most relevant for this trial. Comparison 2 tests the impact of indirect effects over and above direct effects, as might be of interest when the viral effects are of primary importance.\n\n\n\nTable 2.5: Analysis with network effects for the 2016 wheat experiment.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSum Sq\nMean Sq\nNumDF\nDenDF\nF-value\np-value\n\n\n\n\nComparison 1\n\n\n\n\n\n\n\n\nIndirect effect\n32.58\n1.63\n20.00\n31.76\n3.24\n0.0015\n\n\nDirect effect\n19.34\n0.97\n20.00\n35.48\n1.92\n0.0437\n\n\nComparison 2\n\n\n\n\n\n\n\n\nDirect effect\n32.20\n1.61\n20.00\n35.85\n3.20\n0.0012\n\n\nIndirect effect\n20.41\n1.02\n20.00\n32.09\n2.03\n0.0361\n\n\n\n\n\n\nAn optimal design was sought assuming model (2.4) to allow efficient estimation of the direct treatment effects, using the \\(\\phi_\\tau\\)criterion. The design is shown in Figure 2.6. Two features of note are that (i) there is a good spatial spread of treatments, but that (ii) it is also quite common for pairs of connected units to share a treatment. Both these features have been observed in previous row-column and network designs; see Freeman (1979), Parker et al. (2016) and Koutra et al. (2021).\n\n\nCode\ndata.frame(\n  eff = 254 / c(642, 589, 550, 499, 549, 506, 353, 254), \n  design = c(\"CRD\", \"RBD\", \"RCD\", \"BRCD\", \"LND\", \"BND\", \"RCND\", \"BRCND\")\n) |&gt; \n  pivot_wider(names_from = design, values_from = eff) |&gt;\n  mutate(rname = \"Efficiency\") |&gt;\n  column_to_rownames(\"rname\") |&gt;\n  kable(digits = 2) |&gt;\n  add_header_above(c(\"\",\"Designs\" = 8)) \n\n\n\n\nTable 2.6: Efficiencies of optimal designs for various submodels of (2.4) when evaluated under the full model.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDesigns\n\n\n\n\nCRD\nRBD\nRCD\nBRCD\nLND\nBND\nRCND\nBRCND\n\n\n\n\nEfficiency\n0.4\n0.43\n0.46\n0.51\n0.46\n0.5\n0.72\n1\n\n\n\n\n\n\n\n\n\n\nIgnoring block and network effects when designing the experiment can, once again, have a substantial impact on the efficiency of the design if these effects are then including at the analysis stage. Table 2.6 shows the efficiencies under the \\(\\phi_\\tau\\)-criterion for various submodel of (2.4) (BRCND), ranging from a completely randomised design (CRD) that ignores blocks and network effects to a row-column network design that only ignores super-blocks (RCND). All designs lose at least 28% efficiency. Once again, including network effects but ignoring blocks and/or row-columns (LND, BND) is inefficient as the distribution of the treatments across blocks will be sub-optimal, achieving around the same efficiency as the designs which include row-columns and/or blocks but ignore network effects (RCD, BRCD). Clearly, ignoring network effects at the analysis stage, as well as when designing the experiment, could lead to biased estimates of the direct treatment effects.\n\n\n\n\nBesag, J. and Kempton, R. A. (1986) Statistical analysis of field experiments using neighbouring plots. Biometrics, 231–251.\n\n\nCook, R. D. and Nachtsheim, C. J. (1980) A comparison of algorithms for constructing exact d-optimal designs. Technometrics, 315–324.\n\n\nFreeman, G. H. (1979) Some two-dimensional designs balanced for nearest neighbours. Journal of the Royal Statistical Society B, 88–95.\n\n\nKoutra, V., Gilmour, S. G. and Parker, B. M. (2021) Optimal block designs for experiments on networks. Journal of the Royal Statistical Society C, 596–618.\n\n\nKoutra, V., Gilmour, S. G., Parker, B. M., et al. (2023) Design of agricultural field experiments accounting for both complex blocking structures and network effects. Journal of Agricultural, Biological and Environmental Statistics, 526–548.\n\n\nLarsen, N., Stallrich, J., Sengupta, S., et al. (2024) Statistical challenges in online controlled experiments: A review of A/B testing methodology. The American Statistician, 78, 135–149.\n\n\nParker, B. M., Gilmour, S. G. and Schormans, J. (2016) Optimal design of experiments on connected units with application to social networks. Journal of the Royal Statistical Society C, 455–480.",
    "crumbs": [
      "Experiments with interference",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Networked experiments</span>"
    ]
  },
  {
    "objectID": "multi.html",
    "href": "multi.html",
    "title": "Multi-objective experimentation",
    "section": "",
    "text": "Various authors have written summary “checklists” for planning experiments, with items similar to “define the objectives” and “specify the model”. See Dean et al. (2017) for one example. In general, such lists recognise (a) the conflicting nature of many of the former; and (b) the a priori uncertainty in the latter.\nAdopting a multi-objective approach to the design of the experiment directly addresses the first of these concerns, and can also be used to tackle the second problem. In Week 3, you have already seen some discussion of these issues. Broadly speaking, we could use a Pareto, constrained or compound approach to the problem of finding a single experimental design that can tackle a set of, possibly competing, objectives (e.g., Clyde and Chaloner (1996), Lu et al. (2014)).\nFollowing Egorova and Gilmour (2023), Our focus here will be on using multi-objective optimal designs to address uncertainty in the assumed by model by combing individual criteria that target (i) inference for an assumed model; (ii) the ability of identify model lack-of-fit; and (iii) minimum mean squared error, including bias from model misspecification. We will discuss such methods in the context of response surface models of the form\n\\[\n\\begin{split}\n    Y_i & = \\beta_0 + \\sum_{j = 1} ^ {p} \\beta_j x_{ij} + \\varepsilon_{i} \\\\\n    & = \\beta_0 + \\boldsymbol{x}_{i1}^{\\mathrm{T}}\\boldsymbol{\\beta}_1 + \\varepsilon_{i}\\,,\n\\end{split}\n\\tag{1}\\]\nwith \\(\\beta_0\\) and \\(\\boldsymbol{\\beta}_1^{\\mathrm{T}} = (\\beta_1, \\ldots, \\beta_{p})\\) containing unknown parameters to be estimated, and \\(\\boldsymbol{x}_{i1}^{\\mathrm{T}} = (x_{i1}, \\ldots, x_{ip})\\) the values of the \\(p\\) predictors for the \\(i\\)th run. The unit effects (errors) \\(\\varepsilon_{i}\\) have expectation zero and constant variance \\(\\sigma^2\\), with \\(\\varepsilon_{i}, \\varepsilon_{i^\\prime}\\) assumed independent for \\(i \\neq i^\\prime\\). The \\(p\\) predictors may include linear terms in the \\(k\\) factors, higher-order polynomial terms and interactions.\n\n\n\n\nClyde, M. and Chaloner, K. (1996) The equivalence of constrained and weighted designs in multiple objective design problems. Journal of the American Statistical Association, 91, 1236–1244.\n\n\nDean, A., Voss, D. and Draguljić (2017) Design and Analysis of Experiments. 2nd ed. Springer.\n\n\nEgorova, O. and Gilmour, S. G. (2023) Optimal response surface designs in the presence of model contamination. arXiv:2208.05366.\n\n\nLu, L., Anderson-Cook, C. M. and Lin, D. K. J. (2014) Optimal designed experiments using a Pareto front search for focused preference of multiple objectives. Computational Statistics and Data Analysis, 71, 1178–1192.",
    "crumbs": [
      "Multi-objective experimentation"
    ]
  },
  {
    "objectID": "model_robust.html",
    "href": "model_robust.html",
    "title": "3  Model-robust design and lack-of-fit",
    "section": "",
    "text": "3.1 Model-robust design\nStandard optimal designs, as discussed in Week 3, suffer from a dependency on the model assumed at the design stage (Atkinson et al., 2007). In one sense this is natural; it is only by leverage such prior information that we can maximise the efficiency of our experiments. But clearly, this model will usually not be known with certainty before experimentation, particularly in a study where the aim is to fit one or more empirical response surface models, and hence it is desirable to ensure some robustness to the selected model and to enable assessment of lack of fit.\nThe most common design selection criteria aim at estimation or inference for the primary model (1). For estimation of \\(\\boldsymbol{\\beta}_1\\) when \\(\\sigma^2\\) is known, D- and L-optimality are popular that minimise the following functions of design \\(\\mathcal{D}\\), treating the intercept \\(\\beta_0\\) as a nuisance parameter:\n\\[\n\\begin{split}\n\\phi_{D}(\\mathcal{D}) =  \\left|\\left[X_1^{\\mathrm{T}}\\left(I_n - \\frac{1}{n}J_n\\right)X_1\\right]^{-1}\\right|\\,, & \\quad \\mbox{($D$-optimality)} \\\\\n\\phi_L(\\mathcal{D}) =  \\mbox{tr}\\left\\{L^{\\mathrm{T}}\\left(X_1^{\\mathrm{T}}(I_n - \\frac{1}{n}J_n)X_1\\right)^{-1}L\\right\\}\\,. & \\quad \\mbox{($L$-optimality)}\n\\end{split}\n\\]\nHere \\(I_n\\) and \\(J_n\\) are the \\(n\\times n\\) identity and all-ones matrices, respectively, and \\(L\\) is a \\(p_1 \\times p\\) matrix defining \\(p_1\\) linear combinations \\(L^{\\mathrm{T}}\\boldsymbol{\\beta}_1\\) of interest in the experiment.\nBox and Draper (1959) introduced the idea of incorporation of discrepancy between the assumed response surface model and an encompassing “true” model. The encompassing model contained addition polynomial terms; here we assume there are \\(q\\) such terms, typically higher-order monomials, possibly including interactions, which we will label \\(x_{i(p+1)},\\ldots, x_{i(p+q)}\\). The encompassing model can therefore be written as\n\\[\n\\begin{split}\n    Y_i & = \\beta_0 + \\sum_{j = 1} ^ {p} \\beta_j x_{ij} + \\sum_{j = p+1} ^ {p+q} \\beta_j x_{ij} +\\varepsilon_i \\\\\n    & = \\beta_0 + \\boldsymbol{x}_{i1}^{\\mathrm{T}}\\boldsymbol{\\beta}_1 + \\boldsymbol{x}_{i2}^{\\mathrm{T}}\\boldsymbol{\\beta}_2 + \\varepsilon_i\\,,\n\\end{split}\n\\tag{3.1}\\]\nwhere \\(\\boldsymbol{x}_{i2}^{\\mathrm{T}} = (x_{i(p+1)}, \\ldots, x_{i(p+q)})\\) holds the additional \\(q\\) polynomial terms, with associated parameters \\(\\boldsymbol{\\beta}_2^{\\mathrm{T}} = (\\beta_{p+1}, \\ldots, \\beta_{p+q})\\). Model~(1) is clearly a special case of model (3.1) with \\(\\boldsymbol{\\beta}_2 = \\boldsymbol{0}_q\\). DuMouchel and Jones (1994) labelled the polynomial terms in the assumed model as primary and the additional terms in the encompassing model as potential.\nOne desirable aim is to be able to estimate \\(\\boldsymbol{\\beta}_1\\) from model (1) protected from contamination from the potential terms. Assuming least squares estimation, or equivalently maximum likelihood estimation with normal errors, it is natural to consider the mean squared error (MSE) of \\(\\hat{\\boldsymbol{\\beta}}_1\\) which can be characterised via the MSE matrix (Montepiedra and Fedorov, 1997):\n\\[\n\\begin{split}\n\\mbox{MSE}\\left(\\hat{\\boldsymbol{\\beta}}_1\\right)& = \\mathtt{E}_{\\boldsymbol{Y}}[(\\hat{\\boldsymbol{\\beta}}_1 -\\boldsymbol{\\beta}_1)(\\hat{\\boldsymbol{\\beta}}_1 - \\boldsymbol{\\beta}_1)^\\top]\\notag\\\\\n& = \\sigma^2[X_1^{\\mathrm{T}} (I_n - \\frac{1}{n}J_n) X_1]^{-1} + A_1\\boldsymbol{\\beta}_2\\boldsymbol{\\beta}_2^{\\mathrm{T}} A_1^{\\mathrm{T}}\\,,\n\\end{split}\n\\tag{3.2}\\]\nwhere\n\\[\nA_1 = \\left[X_1^{\\mathrm{T}} \\left(I_n - \\frac{1}{n}J_n\\right) X_1\\right]^{-1}X_1^{\\mathrm{T}} \\left(I_n - \\frac{1}{n}J_n\\right)X_2\n\\]\nis the \\(p\\times q\\) alias matrix between the primary and potential terms (excluding the intercept).\nAn analogy of variance-based alphabetic criteria is to consider functionals of this matrix. For the determinant, letting \\(M = X_1^{\\mathrm{T}} (I_n - \\frac{1}{n}J_n) X_1\\) and \\(\\tilde{\\boldsymbol{\\beta}}_2 = \\boldsymbol{\\beta}_2 / \\sigma\\), we obtain\n\\[\n\\begin{split}\n\\left|\\mbox{MSE}\\left(\\hat{\\boldsymbol{\\beta}}_1\\right)\\right| & = \\sigma^{2p}\\left| M^{-1} + A_1\\tilde{\\boldsymbol{\\beta}}_2\\tilde{\\boldsymbol{\\beta}}_2^{\\mathrm{T}} A_1^{\\mathrm{T}}\\right| \\\\\n& = \\sigma^{2p}\\left|M^{-1}\\right|\\left(1 + \\tilde{\\boldsymbol{\\beta}}_2^{\\mathrm{T}} X^{\\mathrm{T}}_2X_1M^{-1}X_1^{\\mathrm{T}} X_2\\tilde{\\boldsymbol{\\beta}}_2\\right)\\,,\n\\end{split}\n\\]\nusing the matrix determinant lemma (Harville, 2006. p. 417). Hence, on a log scale,\n\\[\n\\begin{split}\n\\log \\left|\\mbox{MSE}\\left(\\hat{\\boldsymbol{\\beta}}_1\\right)\\right| = p\\log \\sigma^2 + \\log \\left| M^{-1} \\right| \\\\\n+ \\log\\left(1 + \\tilde{\\boldsymbol{\\beta}}_2^{\\mathrm{T}} X^{\\mathrm{T}}_2X_1M^{-1}X_1^{\\mathrm{T}} X_2\\tilde{\\boldsymbol{\\beta}}_2\\right)\\,.\n\\end{split}\n\\tag{3.3}\\]\nThe first summand is constant with respect to the design, and hence can be excluded from any objective function. The second summand is the (log-scale) D-optimality objective function, focused on precise estimation of the primary terms \\(\\boldsymbol{\\beta}_1\\). And the third summand quantifies the bias introduced by not including the potential terms in the fitted model.\nThe trade-off between variance and bias is clearly controlled by the relative values of \\(\\sigma^2\\) and \\(\\boldsymbol{\\beta}_2\\); in particular, Equation (3.3) reduces to a form equivalent to the D-optimality objective function if \\(\\boldsymbol{\\beta}_2 = \\boldsymbol{0}_q\\).\nIn general, the values of \\(\\boldsymbol{\\beta}_2\\) will not be known. Assuming a prior distribution for \\(\\tilde{\\boldsymbol{\\beta}}_2\\), the expectation of (3.3) can be approximated using Monte Carlo simulation as\n\\[\n\\begin{split}\nE\\left[\\log \\left|\\mbox{MSE}\\left(\\hat{\\boldsymbol{\\beta}}_1\\right)\\right|\\right] & =\np\\log \\sigma^2 + \\log \\left| M^{-1} \\right| \\\\\n& + E\\left[\\log\\left(1 + \\tilde{\\boldsymbol{\\beta}}_2^\\top X^\\top_2X_1M^{-1}X_1^\\top X_2\\tilde{\\boldsymbol{\\beta}}_2\\right)\\right] \\nonumber\\\\\n& \\approx p\\log \\sigma^2 + \\log \\left| M^{-1} \\right| \\\\\n&+ \\frac{1}{B}\\sum_{i=1}^B\\log\\left(1 + \\tilde{\\boldsymbol{\\beta}}_{2i}^\\top X^\\top_2X_1M^{-1}X_1^\\top X_2\\tilde{\\boldsymbol{\\beta}}_{2i}\\right)\\,,\n\\end{split}\n\\tag{3.4}\\]\nwhere \\(\\tilde{\\boldsymbol{\\beta}}_{21}, \\ldots,\\tilde{\\boldsymbol{\\beta}}_{2B}\\) are a sample from the prior distribution. Here, we will use a normal distribution for \\(\\boldsymbol{\\beta}_2\\sim \\mathcal{N}\\left(\\boldsymbol{0}_q, \\sigma^2\\tau^2I_q\\right)\\) for \\(\\tau^2&gt;0\\), leading to \\(\\tilde{\\boldsymbol{\\beta}}_2\\sim \\mathcal{N}(\\boldsymbol{0}_q,\\tau^2I_q)\\).\nObtaining a precise approximation via (3.4) may require large values of \\(B\\) and hence be computationally expensive. As an alternative, we can take a “locally optimal” approach and choose a point prior for \\(\\tilde{\\boldsymbol{\\beta}}_2\\) at which to evaluate (3.3). One possibility is to set \\(\\boldsymbol{\\beta}_2 = \\pm \\sigma\\tau \\boldsymbol{1}_q\\), and hence \\(\\tilde{\\boldsymbol{\\beta}}_2 = \\pm \\tau \\boldsymbol{1}_q\\). This choice fixes each potential parameter to be one standard deviation from the prior mean. Taking \\(\\tilde{\\boldsymbol{\\beta}}_2 = \\tau \\boldsymbol{1}_q\\), without loss of generality, we obtain\n\\[\n\\begin{split}\nE\\left[\\log \\left|\\mbox{MSE}\\left(\\hat{\\boldsymbol{\\beta}}_1\\right)\\right|\\right] \\approx\np\\log \\sigma^2 + \\log \\left| M^{-1} \\right| \\\\\n+ \\log\\left(1 + \\tau^2\\boldsymbol{1}_q^{\\mathrm{T}} X^{\\mathrm{T}}_2X_1M^{-1}X_1^{\\mathrm{T}} X_2\\boldsymbol{1}_q\\right)\\,.\n\\end{split}\n\\tag{3.5}\\]\nWe define the MSE(D)-criterion via minimisation of\n\\[\n\\phi_{MSE(D)}(\\mathcal{D}) = \\exp\\left\\{E\\left[\\log \\left|\\mbox{MSE}\\left(\\hat{\\boldsymbol{\\beta}}_1\\right)\\right|\\right]\\right\\}\\,,\n\\]\nwith the expected log determinant approximated using either (3.4) or (3.5).\nWe can also consider an MSE(L)-criterion formed from the trace of the MSE matrix. As a linear functional, the expectation can be found directly:\n\\[\n\\begin{split}\n\\phi_{MSE(L)}(\\mathcal{D}) & = E\\left\\{\\mbox{trace}\\left[\\mbox{MSE}\\left(\\hat{\\boldsymbol{\\beta}}_1\\right)\\right]\\right\\} \\\\\n& = \\mbox{trace}\\left\\{E\\left[\\mbox{MSE}\\left(\\hat{\\boldsymbol{\\beta}}_1\\right)\\right]\\right\\} \\\\\n& = \\mbox{trace}\\left[\\sigma^2M^{-1} + E\\left(A_1\\boldsymbol{\\beta}_2\\boldsymbol{\\beta}_2^\\top A_1^\\top\\right)\\right] \\\\\n& = \\sigma^2\\mbox{trace}\\left[M^{-1} + \\tau^2 A_1^\\top A_1\\right]\\,.\n\\end{split}\n\\]",
    "crumbs": [
      "Multi-objective experimentation",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Model-robust design and lack-of-fit</span>"
    ]
  },
  {
    "objectID": "model_robust.html#pure-error-and-lack-of-fit",
    "href": "model_robust.html#pure-error-and-lack-of-fit",
    "title": "3  Model-robust design and lack-of-fit",
    "section": "3.2 Pure error and lack-of-fit",
    "text": "3.2 Pure error and lack-of-fit\nIn earlier weeks of the course, the decomposition of the residual sum of squares into pure error and lack-of-fit has already been discussed. Inference for model (1) also depends on the availability of an unbiased estimator for \\(\\sigma^2\\). When uncertainty about the assumed model is being acknowledged, it is important that sufficient pure error degrees of freedom exist in the design to provide this unbiased estimate. To ensure this, Gilmour and Trinca (2012) suggested a class of criteria that explicitly incorporate the F-distribution quantiles on which parameter confidence regions depend. In particular, they defined DP- and LP-optimal designs that minimise\n\\[\\begin{align*}\n\\phi_{(DP)_S}(\\mathcal{D}) & =  F_{p,d;1-\\alpha}^p\\phi_{D}(\\mathcal{D})\\,, & \\quad \\mbox{($DP$-optimality)} \\\\\n\\phi_{LP}(\\mathcal{D}) & =  F_{1,d;1-\\alpha}\\phi_L(\\mathcal{D})\\,, & \\quad \\mbox{($LP$-optimality)}\n\\end{align*}\\]\nwhere \\(d = n-t\\) is the number of replicated treatments in the experiment, \\(\\alpha\\) is a pre-chosen significance level and \\(F_{df1, df2; 1-\\alpha}\\) is the quantile of an F-distribution with \\(df1\\) and \\(df2\\) degrees of freedom such that the probability of being less than or equal to this quantile is \\(1-\\alpha\\).\nThe alternative experimental aim of model sensitivity is concerned with determining lack-of-fit in the direction of the potential terms. One route to achieving this is via alphabetic criterion based on the posterior variance-covariance matrix for \\(\\boldsymbol{\\beta}_2\\), conditional on the value of \\(\\sigma^2\\), using the prior defined in Section 3.1\n\\[\\begin{align*}\n\\Sigma_2 & = \\sigma^2\\left\\{X_2^{\\mathrm{T}}\\left[I_q - X(X^{\\mathrm{T}} X)^{-1}X^{\\mathrm{T}}\\right]X_2 + \\frac{1}{\\tau^2}I_q\\right\\}^{-1} \\\\\n& = \\sigma^2\\left(R + \\frac{1}{\\tau^2}I_q \\right)^{-1}\\,,\n\\end{align*}\\]\nwhere we assume normally-distributed errors in model (3.1).\nThe ability of the design to make inference about the potential terms, and hence detect any lack of fit in the direction of model (3.1) can be quantified via functionals of \\(R + \\frac{1}{\\tau^2}I_q\\). We define Lack-of-fit DP- and LP-criteria that minimise\n\\[\n\\begin{split}\n\\phi_{LoF-DP}(\\mathcal{D}) = F^q_{q, d; 1-\\alpha_{L}} \\left|R + \\frac{1}{\\tau^2}I_q\\right|^{-1}\\,, & \\quad (\\mbox{LoF-$DP$-optimality)} \\\\\n\\phi_{LoF-LP}(\\mathcal{D}) = F_{1, d; 1-\\alpha_{L}} \\mbox{tr}\\left\\{L^\\top\\left(R + \\frac{1}{\\tau^2}I_q\\right)^{-1}L\\right\\}\\,. & \\quad (\\mbox{LoF-$LP$-optimality)}\n\\end{split}\n\\]\nThe inclusion of the \\(F\\)-quantiles recognises the need for a pure error estimate for \\(\\sigma^2\\) and encourages replication of treatments. Both criteria target designs with matrices \\(X_1\\) and \\(X_2\\) being (near) orthogonal to each other, which will also maximise the power of the lack-of-fit test for the potential terms.\n\n\n\n\nAtkinson, A. C., Donev, A. N. and Tobias, R. D. (2007) Optimum Experimental Designs, with SAS. Oxford University Press.\n\n\nBox, G. E. P. and Draper, N. R. (1959) A basis for the selection of a response surface design. Journal of the American Statistical Association, 54, 622–654.\n\n\nDuMouchel, W. and Jones, B. (1994) A simple Bayesian modification of D-optimal designs to reduce dependence on an assumed model. Technometrics, 36, 37–47.\n\n\nGilmour, S. G. and Trinca, L. A. (2012) Optimum design of experiments for statistical inference (with discussion). Journal of the Royal Statistical Society C, 61, 345–401.\n\n\nHarville, D. A. (2006) Matrix Algebra from a Statistician’s Perspective. Springer New York.\n\n\nMontepiedra, G. and Fedorov, V. V. (1997) Minimum bias designs with constraints. Journal of Statistical Planning and Inference, 63, 97–111.",
    "crumbs": [
      "Multi-objective experimentation",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Model-robust design and lack-of-fit</span>"
    ]
  },
  {
    "objectID": "moode.html",
    "href": "moode.html",
    "title": "4  Multi-objective optimal design of experiments (MOODE)",
    "section": "",
    "text": "4.1 Criteria for multiple objectives\nCompound criteria can be combined to find optimal designs under a combination of the criteria for estimation and inference, model sensitivity and model robustness described in Chapter 3. A compound criterion objective function is constructed via a product of individual objective functions, each raised to the power of a non-negative weight (or, equivalently, a product of weighted efficiencies).\nEgorova and Gilmour (2023) defined determinant- and trace-based compound criteria as\n\\[\n\\phi_{det}(\\mathcal{D}) = \\phi_{DP}(\\mathcal{D})^{\\kappa_{DP}}\\times \\phi_{LoF-DP}(\\mathcal{D})^{\\kappa_{LoF-DP}} \\times \\phi_{MSE(D)}(\\mathcal{D})^{\\kappa_{MSE(D)}}\n\\tag{4.1}\\]\nand\n\\[\n\\phi_{trace}(\\mathcal{D}) = \\phi_{LP}(\\mathcal{D})^{\\kappa_{LP}}\\times \\phi_{LoF-LP}(\\mathcal{D})^{\\kappa_{LoF-LP}} \\times \\phi_{MSE(L)}(\\mathcal{D})^{\\kappa_{MSE(L)}}\\,,\n\\tag{4.2}\\]\nrespectively, with all weights \\(\\kappa \\ge 0\\), \\(\\kappa_{DP} + \\kappa_{LoF-DP} + \\kappa_{MSE(D)} = 1\\) and \\(\\kappa_{LP} + \\kappa_{LoF-LP} + \\kappa_{MSE(L)} = 1\\).\nThese compound criteria, along with their componenet criteria, are implemented in the R package MOODE (Koutra et al., 2024), available on CRAN.",
    "crumbs": [
      "Multi-objective experimentation",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Multi-objective optimal design of experiments (MOODE)</span>"
    ]
  },
  {
    "objectID": "moode.html#moode-for-model-robust-inference",
    "href": "moode.html#moode-for-model-robust-inference",
    "title": "4  Multi-objective optimal design of experiments (MOODE)",
    "section": "4.2 MOODE for model-robust inference",
    "text": "4.2 MOODE for model-robust inference\nTo demonstrate MOODE methodology and its implementation in R, we consider an example based around the 12-run Plackett-Burman design. This design is perhaps the most widely used, and studied, non-regular fractional factorial design. It can accommodate up to 11 two-level factors for the orthogonal estimation of main effects. The main effect estimator for each factor is biased by all two-factor interactions not involving that factor, with aliasing coefficients (entries of \\(A_1\\)) given by \\(\\pm 1/3\\).\nHere, we find alternative two-level designs for \\(k=3,\\ldots,9\\) factors using the trace-based compound criterion that minimizes (4.2), under five different sets of criteria weights (Table 4.1). We will also compare against the Placket-Burman designs for each value of \\(k\\), which are \\(L\\)-optimal under a main-effects model.\n\n\n\nTable 4.1: Individual criteria weights for five different compund criteria.\n\n\n\n\n\n\\(\\kappa_1\\)\n\\(\\kappa_2\\)\n\\(\\kappa_2\\)\n\n\n\n\n0.33\n0.33\n0.33\n\n\n0.25\n0.25\n0.5\n\n\n1\n0\n0\n\n\n0\n1\n0\n\n\n0\n0\n1\n\n\n\n\n\n\n\n\nCode\nset.seed(10122024)\nkappa2 &lt;- matrix(\n  c(1/3, 1/3, 1/3,\n    0.25, 0.25, 0.5,\n    1, 0, 0,\n    0, 1, 0,\n    0, 0, 1,\n    0, 0, 0),\n  ncol = 3, byrow = T)    \n\n\nThe primary model considered consists of all \\(k\\) main effects, corresponding to typical use of a Plackett-Burman design, with the potential model also including all two-factor interactions. The MOODE package can be used to find designs under these models and criteria.\n\n\nCode\ndesigns_pb &lt;- list()\nmood_pb &lt;- list()\nplan(multisession)\nfor(j in 3:9) {\n  temp &lt;- list()\n  pb &lt;- list()\n  for(i in 1:nrow(kappa2)) {\n    if(sum(kappa2[i, ]) == 0) { \n      X1 &lt;- undesign(pb(nruns = 12, nfactors = j)) |&gt;\n        mutate(across(all_of(1:j), ~ 2 * as.numeric(.x) - 3)) |&gt;\n        mutate(trt = 1:12, intercept = rep(1, 12), .before = A)\n      temp[[i]] &lt;- list(X1 = as.matrix(X1))\n      temp[[i]]$X2 &lt;- model.matrix(~ (.)^2, X1)[, -(1:(j + 1))]\n    } else { \n      pb[[i]] &lt;- mood(K = j, Levels = 2, Nruns = 12, \n                      criterion.choice = \"MSE.L\",\n                      kappa = list(kappa.LP = kappa2[i, 1], \n                                   kappa.LoF = kappa2[i, 2], \n                                   kappa.mse = kappa2[i, 3]), \n                      model_terms = list(primary.model = \"main_effects\", \n                                         potential.model = \n                                                  \"linear_interactions\"),\n                      control = list(Nstarts = 200))\n      Search_pb &lt;- Search(pb[[i]], parallel = TRUE, verbose = FALSE, \n                          algorithm = \"ptex\")\n      temp[[i]] &lt;- Search_pb\n    }\n  }\n  designs_pb[[j]] &lt;- temp\n  mood_pb[[j]] &lt;- pb\n}\nplan(sequential)  \n\n\n\n\nCode\npb_results &lt;- matrix(NA, nrow = nrow(kappa2) * 7, ncol = 9)\ncount &lt;- 0\nfor (i in 3:9) {\n  for (j in 1:nrow(kappa2)) {\n    kappa.vec &lt;- c(1/3, 1/3, 1/3) \n    pb_temp &lt;- mood(K = i, Levels = 2, Nruns = 12, criterion.choice = \"MSE.L\",\n                    kappa = list(kappa.LP = kappa.vec[1], \n                                 kappa.LoF = kappa.vec[2], \n                                 kappa.mse = kappa.vec[3]), \n                    model_terms = list(primary.model = \"main_effects\", \n                                       potential.model = \"linear_interactions\"))\n    pb_results[count + j, 1] &lt;- i\n    pb_results[count + j, 2:4] &lt;- kappa2[j, ]\n    X1 &lt;- designs_pb[[i]][[j]]$X1\n    X2 &lt;- designs_pb[[i]][[j]]$X2\n    critvals &lt;- MOODE:::icriteria.mseL(X1, X2, pb_temp) \n    pb_results[count + j, 5] &lt;- critvals$LP\n    pb_results[count + j, 6] &lt;- critvals$LoF\n    pb_results[count + j, 7] &lt;- critvals$mse\n    pb_results[count + j, 8] &lt;- critvals$df\n    pb_results[count + j, 9] &lt;- critvals$L\n  }\n  count &lt;- count + 6\n}\npb_results[, 5] &lt;- 100 * ifelse(pb_results[, 5] == 0, 0, rep(pb_results[seq(3, 42, by = 6), 5], rep(6, 7)) / pb_results[, 5]) \npb_results[, 6] &lt;- 100 * ifelse(pb_results[, 6] == 0, 0, rep(pb_results[seq(4, 43, by = 6), 6], rep(6, 7)) / pb_results[, 6]) \npb_results[, 7] &lt;- 100 * ifelse(pb_results[, 7] == 0, 0, rep(pb_results[seq(5, 44, by = 6), 7], rep(6, 7)) / pb_results[, 7]) \npb_results[, 9] &lt;- 100 * ifelse(pb_results[, 9] == 0, 0,  ((1 / 12) * (rep(3:9, rep(6, 7))) / (rep(4:10, rep(6, 7))))  / pb_results[, 9])\ncolnames(pb_results) &lt;- c(\"K\", \"kappa1\", \"kappa2\", \"kappa3\", \"LP\", \"LoF\", \"MSE\", \"DF\", \"L\")\n\npb_results &lt;- data.frame(pb_results)\n\npb_results &lt;- pb_results |&gt;\n  mutate(design = case_when(\n    kappa1 == 1/3 ~ \"Compound 1\",\n    kappa1 == 0.25 ~ \"Compound 2\",\n    kappa1 == 1 ~ \"LP\",\n    kappa2 == 1 ~ \"LoF\",\n    kappa3 == 1 ~ \"MSE\",\n    kappa1 + kappa2 + kappa3 == 0 ~ \"L\"\n  ))\n\nLP_plot &lt;- pb_results |&gt;\n  subset(K &lt; 11) |&gt;\n  ggplot(aes(x = K, y = LP, colour = design, shape = design)) +\n  geom_jitter(height = 0, width = .1) +\n  labs(x = \"k\") +\n  scale_y_continuous(breaks = c(0, 25, 50, 75, 100), \n                     limits = c(0, 130))\nMSE_plot &lt;- pb_results |&gt;\n  subset(K &lt; 11) |&gt;\n  ggplot(aes(x = K, y = MSE, colour = design, shape = design)) +\n  geom_jitter(height = 0, width = .1) +\n  labs(x = \"k\") +\n  scale_y_continuous(breaks = c(0, 25, 50, 75, 100), \n                     limits = c(0, 130))\nDF_plot &lt;- pb_results |&gt;\n  subset(K &lt; 11) |&gt;\n  ggplot(aes(x = K, y = DF, colour = design, shape = design)) +\n  geom_jitter(height = 0, width = .1) +\n  labs(x = \"k\", y = \"PE\")\nLs_plot &lt;- pb_results |&gt;\n  subset(K &lt; 11) |&gt;\n  ggplot(aes(x = K, y = L, colour = design, shape = design)) +\n  geom_jitter(height = 0, width = .1) +\n  labs(x = \"k\") +\n  scale_y_continuous(breaks = c(0, 25, 50, 75, 100), \n                     limits = c(0, 130))\nqp &lt;- LP_plot + MSE_plot + DF_plot + Ls_plot + plot_layout(guides = \"collect\")\nqp\n\n\n\n\n\n\n\n\nFigure 4.1: Efficiencies for MOODE and other optimal designs under different criteria.\n\n\n\n\n\nThe efficiencies of the resulting designs are displayed in Figure 4.1 under the LP-, MSE(L)- and L-criteria, along with the pure error (PE) degrees of freedom. As noted above, the L-optimal designs are simply 12-run Plackett-Burman designs (subsets of columns of the full designs). These designs tend to lack replication, and hence have few or no PE degrees of freedom. This lack of PE degrees of freedom leads to zero efficiency under the LP-criterion. For \\(k&gt;4\\), the MSE(L)-optimal designs also lack pure error degrees of freedom, leading to zero LP-efficiency. The compound designs achieve a minimum of 50% LP-efficiency, and typically achieve somewhat higher, especially for larger numbers of factors. They tend to have four PE degrees of freedom when possible, dropping as the number of factors increases (when the fixed experiment size limits the number of degrees of freedom possible). Also note that for \\(k=7\\), the algorithm found a better design under the LP-criterion when searching for the compound designs; this indicates the difficulty sometimes found in finding “pure error” optimal designs, and could possibly be mitigated by more random starts or a more sophisticated choice of starting designs.\nWe see more complex patterns under the MSE(L)-criterion. When \\(k\\) is smaller, there is very much less replication in the MSE(L)-optimal designs than for the LP-optimal designs, leading to low MSE(L)-efficiency for these latter designs. The compound designs start with very high efficiency for \\(k=3,4\\) but they both maintain replication and PE degrees of freedom for \\(k=5,6\\) leading to much lower efficiency. As \\(k\\) increases, the scope for replication decreases due to the fixed experiment size, and the efficiency of the LP-optimal and compound design improves. The L-optimal designs perform poorly under the MSE(L)-criterion. However, it should be noted that there are multiple choices of the subsets of columns of the Plackett-Burman design, each of which is L-optimal but may have quite different performance under the other criteria.\nUnder L-optimality, efficiencies for all designs, excluding the LoF design, lie above 50%, with highest efficiencies being for \\(k=3\\). The LoF designs tend to perform poorly under all other criteria for \\(k&gt;3\\), and in fact the designs found under LP-optimality typically had LoF efficiency near or at 100%. Conversely, for these limited run sizes and low run-size to number-of-factors ratios, the designs which were optimal under both the LP- and LoF-criteria were never found from directly optimizing the LoF criteria.\nIn general, the F-quantiles in the LP-optimality are large for the small PE degrees of freedom possible here. Hence for small values of \\(k\\), where replication is possible, the LP-designs are dominated by this quantile, and we see poorer performance of these designs under the MSE(L)-criterion. For larger values of \\(k\\), only very limited replication is possible and so the designs, and their performances under different criteria, tend to converge.\nThe compound design for \\(\\kappa_{LP} = \\kappa_{LoF-LP} = \\kappa_{MSE(L)} = 1/3\\) and \\(k=4\\) is given in Table 4.2, together with the corresponding LP- and MSE(L)-optimal designs. The relative importance of replication to these three criteria is clear, with the LP-optimal design having only 5 distinct points (the minimum number possible), compared to 8 distinct points for the compound design and 12 (obviously the maximum) for the MSE(L)-optimal design. The compound design only includes treatments also in the other two designs. None of these designs are orthogonal in the main effects, a property of the Plackett-Burman design that is compromised to obtain either PE degrees of freedom or, in the case of the compound and MSE(L)-design, better robustness from the potential terms. These latter two designs achieve orthogonality between the main effects and two-factor interactions, i.e. \\(A_1\\) is a zero matrix. Of course, the relative importance of the aliasing is controlled by the choice of \\(\\tau^2\\), the prior variance for the potential terms, which is here set to the default of \\(\\tau^2=1\\).\n\n\nCode\nX11 &lt;- designs_pb[[4]][[1]]$X1\nX12 &lt;- designs_pb[[4]][[2]]$X1\nX13 &lt;- designs_pb[[4]][[3]]$X1\nX15 &lt;- designs_pb[[4]][[5]]$X1\nX16 &lt;- designs_pb[[4]][[6]]$X1\n\npbtab &lt;- kable(round(cbind(X11[order(X11[,1]), c(1, 3:6)], X13[order(X13[,1]), c(1, 3:6)], X15[order(X15[,1]), c(1, 3:6)]), digits = 2), \n             col.names = \n               rep(c(\"Trt label\",\"$x_{1}$\", \"$x_{2}$\", \"$x_{3}$\", \"$x_{4}$\"), 3),\n             escape = FALSE, booktabs = T,\n             linesep = \"\") \n#|&gt;\n#  add_header_above(header = c(\"Compound 1\" = 5, \"$LP$-optimal\" = 5, \"$MSE(L)$-optimal\" = 5), escape = F) \npbtab\n\n\n\n\nTable 4.2: Compound optimal design (left) for \\(k=4\\) two-level factors with \\(n=12\\) runs and \\(\\kappa_{LP} = \\kappa_{LoF-LP} = \\kappa_{MSE(L)} = 1/3\\), along with the corresponding LP-optimal (middle) and MSE(L)-optimal (right) designs.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTrt label\n\\(x_{1}\\)\n\\(x_{2}\\)\n\\(x_{3}\\)\n\\(x_{4}\\)\nTrt label\n\\(x_{1}\\)\n\\(x_{2}\\)\n\\(x_{3}\\)\n\\(x_{4}\\)\nTrt label\n\\(x_{1}\\)\n\\(x_{2}\\)\n\\(x_{3}\\)\n\\(x_{4}\\)\n\n\n\n\n1\n-1\n-1\n-1\n-1\n2\n-1\n-1\n-1\n1\n1\n-1\n-1\n-1\n-1\n\n\n4\n-1\n-1\n1\n1\n2\n-1\n-1\n-1\n1\n2\n-1\n-1\n-1\n1\n\n\n6\n-1\n1\n-1\n1\n2\n-1\n-1\n-1\n1\n3\n-1\n-1\n1\n-1\n\n\n6\n-1\n1\n-1\n1\n7\n-1\n1\n1\n-1\n5\n-1\n1\n-1\n-1\n\n\n7\n-1\n1\n1\n-1\n7\n-1\n1\n1\n-1\n6\n-1\n1\n-1\n1\n\n\n7\n-1\n1\n1\n-1\n9\n1\n-1\n-1\n-1\n8\n-1\n1\n1\n1\n\n\n10\n1\n-1\n-1\n1\n9\n1\n-1\n-1\n-1\n9\n1\n-1\n-1\n-1\n\n\n10\n1\n-1\n-1\n1\n9\n1\n-1\n-1\n-1\n11\n1\n-1\n1\n-1\n\n\n11\n1\n-1\n1\n-1\n12\n1\n-1\n1\n1\n12\n1\n-1\n1\n1\n\n\n11\n1\n-1\n1\n-1\n12\n1\n-1\n1\n1\n14\n1\n1\n-1\n1\n\n\n13\n1\n1\n-1\n-1\n14\n1\n1\n-1\n1\n15\n1\n1\n1\n-1\n\n\n16\n1\n1\n1\n1\n14\n1\n1\n-1\n1\n16\n1\n1\n1\n1\n\n\n\n\n\n\n\n\n\n\n\n\nEgorova, O. and Gilmour, S. G. (2023) Optimal response surface designs in the presence of model contamination. arXiv:2208.05366.\n\n\nKoutra, V., Egorova, O., Gilmour, S. G., et al. (2024) MOODE: An r package for multi-objective optimal design of experiments. Available at: https://arxiv.org/abs/2412.17158.",
    "crumbs": [
      "Multi-objective experimentation",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Multi-objective optimal design of experiments (MOODE)</span>"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "Summary",
    "section": "",
    "text": "In Week 4, we have introduced two areas of active research in design of experiments, that build directly on core material from earlier weeks. Both topics, treatment inference and multi-objective design, reduce to some extent the reliance of experimental design methodologies on assumptions that may be unrealistic in many cases. Clearly, these two topics could also be combined. e.g., multi-objective designs could be sought for networked experiments to estimate both direct and indirect treatment effects.\nThese topics also intersect with other currently trending research areas in design of experiments. The design and analysis of experiments on networks is also an active area within the causal inference community (e.g., Hudgens and Halloran, 2008); a workshop was held at King’s in the summer of 2024. The approach taken by this community tends to differ somewhat from that outlined here, and makes more use of randomisation-based inference. However, many of the issues and objectives are similar.\nIncreasingly, experiments are taking place on very large networks, particularly online experimentation e.g., on social media (e.g., Nandy et al., 2020). Direct application of the methods in Section 2 will usually be prohibitively computationallyn expensive in such cases. Connections can be made to methods for subsampling large data using design of experiments principles, e.g., Yu et al. (2024).\n\n\n\n\nHudgens, M. G. and Halloran, M. E. (2008) Toward causal inference with interference. Journal of the American Statistical Association, 103, 832–842.\n\n\nNandy, P., Basu, K., Chatterjee, S., et al. (2020) A/B testing in dense large-scale networks: Design and inference. In: 2020.\n\n\nYu, J., Ai, M. and Ye, Z. (2024) A review on design inspired subsampling for big data. Statistical Papers, 65, 467–510.",
    "crumbs": [
      "Summary"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Atkinson, A. C., Donev, A. N. and Tobias, R. D. (2007) Optimum\nExperimental Designs, with SAS. Oxford University\nPress.\n\n\nBesag, J. and Kempton, R. A. (1986) Statistical analysis of field\nexperiments using neighbouring plots. Biometrics, 231–251.\n\n\nBose, M. and Dey, A. (2015) Crossover designs. In Handbook of the\nDesign and Analysis of Experiments, pp. 159–196.\n\n\nBox, G. E. P. and Draper, N. R. (1959) A basis for the selection of a\nresponse surface design. Journal of the American Statistical\nAssociation, 54, 622–654.\n\n\nClyde, M. and Chaloner, K. (1996) The equivalence of constrained and\nweighted designs in multiple objective design problems. Journal of\nthe American Statistical Association, 91,\n1236–1244.\n\n\nCook, R. D. and Nachtsheim, C. J. (1980) A comparison of algorithms for\nconstructing exact d-optimal designs. Technometrics, 315–324.\n\n\nCox, D. R. (1958) Planning of Experiments. Wiley.\n\n\nDean, A., Voss, D. and Draguljić (2017) Design and Analysis of\nExperiments. 2nd ed. Springer.\n\n\nDuMouchel, W. and Jones, B. (1994) A simple Bayesian\nmodification of D-optimal designs to reduce dependence on\nan assumed model. Technometrics, 36, 37–47.\n\n\nEgorova, O. and Gilmour, S. G. (2023) Optimal response surface designs\nin the presence of model contamination. arXiv:2208.05366.\n\n\nFreeman, G. H. (1979) Some two-dimensional designs balanced for nearest\nneighbours. Journal of the Royal Statistical Society B, 88–95.\n\n\nGilmour, S. G. and Trinca, L. A. (2012) Optimum design of experiments\nfor statistical inference (with discussion). Journal of the Royal\nStatistical Society C, 61, 345–401.\n\n\nHarville, D. A. (2006) Matrix Algebra from a Statistician’s\nPerspective. Springer New York.\n\n\nHudgens, M. G. and Halloran, M. E. (2008) Toward causal inference with\ninterference. Journal of the American Statistical Association,\n103, 832–842.\n\n\nJohn, P. W. (1971) Statistical Design and Analysis of\nExperiments. Macmillan.\n\n\nJones, B. and Kenwood, M. G. (2015) Design and Analysis of\nCross-over Trials. Chapman & Hall/CRC Press.\n\n\nKoutra, V., Gilmour, S. G. and Parker, B. M. (2021) Optimal block\ndesigns for experiments on networks. Journal of the Royal\nStatistical Society C, 596–618.\n\n\nKoutra, V., Gilmour, S. G., Parker, B. M., et al. (2023) Design of\nagricultural field experiments accounting for both complex blocking\nstructures and network effects. Journal of Agricultural, Biological\nand Environmental Statistics, 526–548.\n\n\nKoutra, V., Egorova, O., Gilmour, S. G., et al. (2024) MOODE: An r\npackage for multi-objective optimal design of experiments. Available at:\nhttps://arxiv.org/abs/2412.17158.\n\n\nLarsen, N., Stallrich, J., Sengupta, S., et al. (2024) Statistical\nchallenges in online controlled experiments: A review of\nA/B testing methodology. The American\nStatistician, 78, 135–149.\n\n\nLu, L., Anderson-Cook, C. M. and Lin, D. K. J. (2014) Optimal designed\nexperiments using a Pareto front search for focused\npreference of multiple objectives. Computational Statistics and Data\nAnalysis, 71, 1178–1192.\n\n\nLucas, H. L. (1957) Extra-period latin-square change over designs.\nJournal of Dairy Science, 40, 225–239.\n\n\nMontepiedra, G. and Fedorov, V. V. (1997) Minimum bias designs with\nconstraints. Journal of Statistical Planning and Inference,\n63, 97–111.\n\n\nNandy, P., Basu, K., Chatterjee, S., et al. (2020) A/B\ntesting in dense large-scale networks: Design and inference. In: 2020.\n\n\nParker, B. M., Gilmour, S. G. and Schormans, J. (2016) Optimal design of\nexperiments on connected units with application to social networks.\nJournal of the Royal Statistical Society C, 455–480.\n\n\nWilliams, E. J. (1949) Experimental designs balanced for the estimation\nof residual effects of treatments. Australian Journal of Scientific\nResearch, 2, 149–168.\n\n\nYu, J., Ai, M. and Ye, Z. (2024) A review on design inspired subsampling\nfor big data. Statistical Papers, 65, 467–510.",
    "crumbs": [
      "References"
    ]
  }
]